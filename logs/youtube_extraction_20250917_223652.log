2025-09-17 22:36:52,020 - INFO - app.youtube.run.20250917_223652 - Iniciando extração em lote de 1 canais
2025-09-17 22:36:52,020 - INFO - app.youtube.run.20250917_223652 - Iniciando extração em lote de 1 canais
2025-09-17 22:36:52,039 - INFO - app.youtube.run.20250917_223652 - Processando canal 1/1: @SuperHumansLife
2025-09-17 22:36:52,039 - INFO - app.youtube.run.20250917_223652 - Processando canal 1/1: @SuperHumansLife
2025-09-17 22:36:52,754 - INFO - youtube_extractor - Extraindo vídeos do canal @SuperHumansLife (max_age_days=3, videos_tab_only=True, max_videos=1)
2025-09-17 22:36:53,036 - INFO - youtube_extractor - Aba /videos: analisados=1, válidos=1, ignorados: live=0, upcoming=0, sem_data=0, antigos=0, shelves=0
2025-09-17 22:36:53,036 - INFO - youtube_extractor - Aba /videos: 1 vídeos válidos (≤ 3 dias)
2025-09-17 22:36:53,037 - INFO - app.youtube.run.20250917_223652 - Canal @SuperHumansLife extraído com sucesso. Vídeos dentro do critério: 1
2025-09-17 22:36:53,037 - INFO - app.youtube.run.20250917_223652 - Canal @SuperHumansLife extraído com sucesso. Vídeos dentro do critério: 1
2025-09-17 22:36:54,767 - INFO - youtube_extractor - Transcrição: disponíveis ['en(auto)'] para 1_0OZvSjabM
2025-09-17 22:36:54,869 - INFO - youtube_extractor - Transcrição encontrada (18667 chars, en) para 1_0OZvSjabM
2025-09-17 22:36:55,969 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:55,971 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 8000 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:56,374 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:56,376 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 6000 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:56,782 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:56,784 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 4000 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:57,192 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:57,194 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 2500 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:57,476 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:57,479 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 1500 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:57,806 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:57,807 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 900 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:58,113 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:58,115 - WARNING - app.domain.llm_client - [LLM] Erro durante chamada ao modelo (trecho 600 chars): Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:58,115 - WARNING - app.domain.llm_client - [LLM] Falha na chamada após múltiplas tentativas: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:58,422 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:58,424 - WARNING - app.domain.llm_client - [LLM] Falha na tradução simples: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:58,658 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:58,658 - WARNING - app.domain.llm_client - [LLM] Falha na tradução simples: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:58,983 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:58,985 - WARNING - app.domain.llm_client - [LLM] Falha na tradução simples: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:59,284 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:59,286 - WARNING - app.domain.llm_client - [LLM] Falha na tradução simples: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-09-17 22:36:59,553 - INFO - httpx - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-17 22:36:59,555 - WARNING - app.domain.llm_client - [LLM] Falha na tradução simples de palavras-chave: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}

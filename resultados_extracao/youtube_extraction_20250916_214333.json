{
  "executed_at": "2025-09-16T21:43:33.341072",
  "mode": "full",
  "total_channels": 2,
  "total_videos": 8,
  "params": {
    "days": 5,
    "max_videos": 30,
    "mode": "full",
    "no_llm": false,
    "asr_provider": "faster-whisper",
    "format": "txt"
  },
  "channels": [
    {
      "channel_id": "@Mark_Kashef",
      "name": "@Mark_Kashef",
      "subscriber_count": "",
      "description": "",
      "video_count": "",
      "videos": [
        {
          "id": "mCXuXmnIvk0",
          "title": "ESTE truque de código do Claude constrói servidores MCP INSTANTANEAMENTE",
          "url": "https://www.youtube.com/watch?v=mCXuXmnIvk0",
          "published": "2025-09-16T00:43:33.968830",
          "published_relative": "há 1 dia",
          "duration": "12:51",
          "date_published": "2025-09-15T14:15:03-07:00",
          "transcript_available": true,
          "transcript": "If you've always wanted to build your own MCP server, but were intimidated at the idea of doing it, this video is for you. I'm going to show you how we can spin up an MCP server that you can generate using Claude code that you can import and use in Claw Desktop immediately. In my last video, I walked through conceptually the three ways that you can use MCPs the easiest right now. But in this one, I'll actually give you what you need to go and start building your own. The thought of building your own server might sound intimidating, but once you see my optimized workflow for getting something up and running in a very short amount of time, you'll be building tons of MCP servers by the end of this video. If you're curious, let's dive right in. All right, so this is Cloud Code and all I've done is create a brand new folder called my MCP up here. And you'll notice I have three files. I have one that's called MCP 101. I have another one that's called MCP complete implementation guide. This is something that I put together. And I have one existing markdown file that's called lazy calculator. So let me explain really quickly what's in each file, where I got it from, and why we're even using it. So the very first file here, if I double click, is called MCP 101. So this is actually coming from an online resource, a free resource that basically acts as a language model cheat sheet of how to use MCPS and where they can or can't be used. So, if you go into Google and you type in model context LLM's full text, you'll get something like this, which is a full text file. If you click on it, it's literally plain text explaining everything. And you can zoom in right here. It has examples of different clients or different platforms and exactly what can or can't be done using MCP servers. So, it goes through methodology, it goes through logic, it goes through prompt engineering. So giving this to a language model, you can see right here different features like built-in MCP servers can quickly be enabled and disabled. It gives a really good primer for the language model to go from having a bachelor's degree in Python to having a master's in using MCP servers. And you can see if we do a control find and we go to claude code, you can see in cloud code, it knows exactly what it is, where it's sourced. If we zoom into this, it tells it what cloud code is. and you can find something specifically referencing in cloud code what can be generated using that tool. So this file is the first piece of the pie, but the second one is the complete implementation guide where I created this using an AI agent to go scrape the web for documentation, YouTube videos, transcripts, and anything that was highly rated related to MCP servers. And this goes through what an MCP server is, three pillars of MCP, different ways to build it, different frameworks to use to build it. So this gives, if not a cheat sheet, a full implementation guide, and when you get stuck, instead of having to go and refer to the web and having cloud code do web search, you can go and just tell it go refer to these two files and try to see what might be happening. And my last file here is called the lazy calculator. Now, I showed this as a small example in my last video at the very end, but basically it is a glorified really bad calculator. And why I include it here is as you build one MCP server, you can use it as long as it works as an example for the next ones you want to build. So, let's say I had five MCP servers. I could throw in another two or threeMD files. And what I usually do is I ask claude code after building a server go and summarize everything that we went through to get to this point so I can use it as like a pointer guide for the next agent that I spin up to build another MCB server. And using this cheat code will let you go from hours to minutes because you have a clear-cut example of what has worked on your specific device. And I say your specific device because when you build an MCP server and you want it to run locally for let's say cloud desktop, you might be on a Windows, you might be on Linux. I'm personally on a Mac from 2023. So each system is going to have small permutations where something does or doesn't work. Once you finally get something to work, make sure you reverse prompt and document it as an artifact so your system can always refer to it to avoid making the same mistakes over and over again. So in that case, let me show you how we can create another MCP server using all this documentation to help us out. So I'm going to use my shortcut command. This is my shortcut command for using clawed dangerously in YOLO mode. I don't recommend this if you're a newbie, but I'm just doing this for speed of this tutorial. Then it spins up. And what I'm going to do is I'm going to do slashinitialize. And what slashinitialize does is it will allow claude code to become familiar with all the resources I've laid out in this codebase and create what's called a claude MD file which will act as the command center brain for operating this entire chat moving forward. So I'll click on this and we'll come back to when it's done. All right. So after a few minutes it's completed its analysis. It's created this MD file to walk itself through all the files we have here and when it can refer to them. You can see right now it's referencing the project structure outlined in the lazy calculator. And now that we have this, I can go back up and let's say we want to make a brand new function. And this function is going to help become a prompt auditor. So we send a request and it goes to a language model. In this case, we can use something like GPT5 Nano and it takes that request and makes it into a proper prompt. Now, could you do this in a language model itself? Yes. But for the purpose of showing you the tutorial, we're going to go with this. So, what I'll do is I'll go into planning mode. I'll do shift tab, go to plan mode. I'll tell exactly what I'm trying to do and I'm going to provide it with the ability to look at the documentation of GBD5. So, let me grab from the OpenAI website using GBD5. There's an example of exactly how to call their API. So, I'm just going to copy that over and then I'll keep that handy for after I dictate. Okay. Okay, so I'm trying to create a brand new MCP server that I want to call prompt assistant. And the whole point of this MCP server is when someone submits a query and invokes this function, it will take the prompt specified in their query that should be basically provided with a variable called prompt equals, then the prompt after will be the part we want to audit. We want to take that and make it into a much more robust version of the prompt using a language model that you're not aware of because your cutoff training was in 2023. It's called GBT5 Nano. I'm going to provide you the documentation on how we're planning on using this. Obviously, you're going to have to spin up something like an environment file for me to put my API key. But once it's there, I want you to use this function or this way of calling a language model specifically. You might default to GP4 because that's what you know, but you're going to use this no matter what. So, come up with a plan of creating the system that sends the initial prompt from wherever we're using this MCP server to this language model and then create a prompt in this language model to take an incoming prompt and act as a prompt engineer and make it a way better prompt and then return that prompt in Markdown. So, we'll send that over. There we go. It'll take few seconds. Now I'll just paste right after it the documentation. I'll say here's the GPT5 nano way to call the API. Then I'll paste this and I'll keep it in plan mode so that I can audit what it comes back with in terms of a possible solution. So it comes back with a full structure for the project. I can see it's using GPT5. This is the structure of the server. So we have the prompt assistant server. We have the run prompt assistant server. We have the test prompts.python file. So this looks decent to me. So I'll click on bypass permissions and then it will go and put together a draft of this. And then the next part is the lazy part where instead of going into cloud desktop and testing it myself, I'm going to go into cloud. So I'll pull this up while this is running. So let me pull up claude. And you'll see when it pops up, we're going to have a normal box here. What I want to go into is I want to click on allow. And then we want to go into settings and then go to developer and then we can go to edit config. And in this case you get routed to the clawed folder. So what you can do here big cheat code is you can rightclick and then copy the path. So in my case I'd have to click option and then when you copy the path to this specific file you can also copy the path to the logs file in claude. And what the logs does is it stores all the errors. So instead of you manually opening Claude, closing it, restarting it, praying it works, you could give it the two file paths and tell it, you know what, once you think you're done, go and open Claude yourself, see if you get errors in the logs. If you get errors in the logs, go and fix those errors and try again until it works. So now you create this feedback loop where Claude code will have its own stimuli to interact with Claude desktop until it's ready to go. And this is a cheat code in terms of time. So, I'm already going to cue up my next prompt here where I tell it, \"Cool, you said you're done.\" Because it's probably going to come back and say, \"I'm done.\" And it works just like it always does. I'm going to paste the two paths. I'm going to tell it open cloud desktop. Test it yourself until you see there's no errors whatsoever. So, it comes back saying it's done. And naturally, I don't trust it. So, I did put my API key in this environment file right here. And then once I did that, I also said go and open up Claw Desktop and test it yourself using these file paths. If you get errors, navigate them in the logs folder and fix those mistakes until it opens and runs error-free. Then I give it the link to both of my paths. So I'm going to send this over and it should autonomously open up Claude from the command line and see whether or not there error. So whilst doing this it should be able to open up cloud desktop on its own just using the terminal checking whether or not there are any errors in the logs associated with opening it up and if there are it should be able to go into that feedback loop until it assesses that at least there aren't any errors. Now whether or not it will functionally work we'll have to test it ourselves. So you can see here it's gone into my cloud desktopconfig.json. It's edited the MCP servers that it has. It's adding the new MCP server and then it's closing the file and opening the brand new environment. All right. So now it says the server started without any errors. Now it's going to verify and restart Cloud Code and check the logs again. So it sees the MCP servers are active. It's going to check whether or not the prompt assistant now pops up in the logs or if it's somehow ignoring it completely. All right. And after it found its own errors, it was able to restart Claude Code. And when I go into claw desktop, I can now see right here we have a few tools. One of them is our brand new prompt assistant. So let's take it for a try. Let me click on this. Let me see what tools we have. So enhanced prompt and test connection. So I'm going to say, okay, I want to be able to adjust this prompt and make it better. So I'm going to write that. I'm going to say prompt equals, can you write me an essay about why AI is cool? Okay. And I send that over. And I send this as a request. Fingers crossed. It should invoke this function. Asks me for permission. And then we'll be able to click on always allow. There we go. Allow always. And we'll see what we get. All right. And it seemed to work. So first it used the enhance prompt. It sent the query that we had right here which was can you write an essay about YI is cool. It comes back with the enhanced prompt in markdown like we asked. It's using GD5 behind the scenes nano specifically. Then it transforms it uses Claude to basically go through the response and we come back with a better version of the prompt that we originally asked for. Now one small thing is that the MCP server did return back the response but Claude didn't give it back to me physically. So I just told it, can you just output what you outputed in that small little dropdown? And then we have the adjusted version of that prompt right here. So in under what 10 15 minutes, we were able to go from a singular prompt, have it go through a feedback loop to check its own work to having a functional MCP server that you own and you own the IP for and the infrastructure on your local computer. And that's pretty much it. This was meant to be a quick and snappy tutorial to unlock a bunch of limiting beliefs that you can do this even if you're not technical and even if you're not a developer. So, what I'll do to make this as easy as possible for you is I'll provide you with the files here on the MCP 101, the implementation guide, as well as the lazy calculator in the second link in the description below to get you started and on your journey to building your own MCP empire. And if you like the way I break down concepts into digestible hacks, then you'll love the infinite amount of hacks that I share with my early AI adopters community. Check out the first link in the description below and maybe I'll see you inside. I'll see you in the next one.",
          "summary": {
            "resumo_uma_frase": "If you've always wanted to build your own MCP server, but were intimidated at the idea of doing it, this video is for you",
            "resumo": "If you've always wanted to build your own MCP server, but were intimidated at the idea of doing it, this video is for you. I'm going to show you how we can spin up an MCP server that you can generate using Claude code that you can import and use in Claw Desktop immediately. In my last video, I walked through conceptually the three ways that you can use MCPs the easiest right now. But in this one, I'll actually give you what you need to go and start building your own. The thought of building your own server might sound intimidating, but once you see my optimized workflow for getting something up and running in a very short amount of time, you'll be building tons of MCP servers by the end of this video. If you're curious, let's dive right in. All right, so this is Cloud Code and",
            "assunto_principal": "ESTE truque de código do Claude constrói servidores MCP INSTANTANEAMENTE",
            "palavras_chave": [
              "actually",
              "always",
              "amount",
              "another",
              "brand",
              "build",
              "building",
              "called",
              "claude",
              "cloud",
              "complete",
              "conceptually"
            ],
            "resumo_em_topicos": "- actually\n- always\n- amount\n- another\n- brand\n- build\n- building\n- called",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "uHU-khKos80",
          "title": "3 maneiras de usar servidores MCP agora mesmo (MODO FÁCIL)",
          "url": "https://www.youtube.com/watch?v=uHU-khKos80",
          "published": "2025-09-15T00:43:33.968972",
          "published_relative": "há 2 dias",
          "duration": "12:27",
          "date_published": "2025-09-14T14:15:02-07:00",
          "transcript_available": true,
          "transcript": "Everyone knows about MCPs by now, but here's what nobody is talking about. 90% of people who know about it still aren't using it or aren't using it effectively. You've probably seen the demos, heard it's the future of AI agents, or even tried to set up one for yourself. But then you hit a wall of, \"Okay, how do I actually use this in my day-to-day workflow?\" After helping dozens of people actually implement MCPs, I've seen the same pattern over and over again. People get stuck between knowing that it exists and actually deriving value from it. So, in this video, I'm going to walk through when it makes sense to use MCPs for your specific use case and three different ways you can think about implementing them. By the end of the video, you'll have clarity on what MCP is, how to use it, and I'll even show you a sneak peek of building your own. Let's get into it. So just to level set so that everyone watching this knows exactly what an MCP server is. You can think of it as the equivalent of a USBC wire. Before we had different wires for different devices and different generations within those devices. But MCP aims to unify all the backends of the different services that you use on a day-to-day basis. Another way to imagine MCP servers is imagine that AI only had the ability to send a chat in a chatbot. But now the AI has eyes. It can physically see different files. It can chat. It can do all kinds of things. And one of my favorite analogies to explain MCP is the world without MCP looks like this, where you had different keys for different doors. And every single time you wanted to make a call to a service, you'd have to find the key in your pile and then go to that door, open it, ask for permission, give a password, then you could walk in. But on the right side, the world of MCP, the goal is to have one golden key that not only opens all of the doors, but opens them and you can go in and out as you please, as long as you give the password the first time. Now, from an engineering standpoint, MCP servers aren't rocket science, and they're not magic. They're basically wrappers on top of existing functions and very elegant ways of calling different services within one tool. And one of the best things about MCP servers is that they can reduce what's called vendor lockin where maybe you've overinvested in an AI platform and all your IP, all your promps, all your knowledge bases live in one area. So if you want that same functionality, you're kind of stuck. By using and building your own MCP servers, you can replicate a lot of that functionality. So you own more of your stack. And more importantly, from a security standpoint, MCP servers aren't known for being very secure on the thirdparty side, but if you build your own, you can make it secure. And in terms of categorizing MCPs in different tiers, this is the way that I like to think about them. So, first you have the easy to implement, usually oneclick with authentication hosted browser MCPS, and I'll show you what they look like just in case in chatbt and claude. But the whole point is that they run in the cloud or in the browser. There's minimal to no secrets outside of maybe an API key of sorts and then there's no local file system access. So the MCP servers can interact with whatever is in the browser in that specific provider. Now the next tier is a very popular one where you can run these MCP servers on your local hardware and recently it actually became a lot easier to do that which I will show you shortly. But the goal of this is that you have more trust, more security. It's not necessarily foolproof still. you'll have to do due diligence to make sure it is indeed secure and more importantly that the MCP server you're using to begin with is secure. But with this option, you can physically interact with the files on your computer as well as the general software. And the last one is more enterprise and larger business scale where you can implement and build your own MCP servers or host existing servers but within your team's infrastructure. So think of the large cloud providers like Microsoft's Azure, Amazon's AWS, Google's GCP. You can actually host these things. So like I said before, you can host these things because they're basically glorified rappers on top of existing functions that you or AI or both of you built together. All right. So if we pop into chatbt, I'm personally on the teams plan here. But if you click on this plus button right here and you click on use connectors and then you click on this little dropdown, you'll see out of the box it comes out with Gmail, Google Drive, Calendar and other integrations as well. And these are basically MCP servers on the browser side where once you authenticate your service, you can use whatever tools they have configured behind the scenes. Not only that, but you can actually add your own MCP servers for existing services that support it. So, for example, Fireflies here interacts with my meeting notetaker app. So, it can go and derive all of the different transcripts and I can use it in my chat to ask questions like what were the biggest objection handle for any of the prompt advisor agency clients we had this week and then on the teams and enterprise plans you can use what's called connect more where you can browse other connectors from HubSpot notion etc or you can actually create your own. So how it works is you'd come up with a name for the connector, then a description, then a URL to the server, but obviously the prerequisite here is that you build the MCP server, which will take a little bit of technical knowhow. On the cloud side, it looks very familiar where if you click on this section and you click on search and tools, you'll see you have the exact same starter MCP servers. And then if you go to add or manage connectors. So, if we go to add, once again, you have a mini app store, pretty large one, and they keep adding to it. So, if you remember what I said about local installations, all of these can interact with things like your file system, your note-taking apps, your iMessage if you're on Mac, anything here can interact with the physical information on your laptop. And once again, if you go to add custom connector, then you'll have the exact same thing where in this case, if you go to advanced settings, you have the name, you have what's called the remote MCP server. Again, this means that it has to be stored somewhere in the cloud so that this can easily access it. Now, if we transition from browserbased to level two localbased, this is still a browser tool, right? It's still clouded, but it's clawed desktop, meaning it does have more access to things like my Docker instance. And Docker essentially lets you run different services in a containerized format. So, it's more secure and you have more control over what it can or can't do. So you'll see here when I go on functions, I'm going to have a few different types of functions. So these two tools here, forecasting calculator and lazy calculator are tools that I physically built myself and I'm hosting locally on my Mac. And just to give you a little demo, lazy calculator is a very dumb app that basically does math but doubles the result. So if I say 4 + 6 and it's actually 10, it will make it 20. So, if I say something like, \"Can you add 6 + 10 but use the lazy calculator tool?\" And I send that over, it should realize that because I said the name, it should pop it up. It'll ask me for permission to use the tool. But I built this myself on my computer just using the eye. So, it's going to come up with the lazy calculation. I think I've already pre-authenticated it before. So, you can see here it's now taken the equation. It said it's 32. And I can walk through the thought process. So, what's 6 + 10? It's 16, but it multiplied it by two, so it's 32. And you can see here, it's a little bit playful where it says wink at the very bottom. But essentially, this is just to show you that it is possible to build and own your own MCP. If I wanted to use this function over here, I would have to find a way to build it in a way that it can be accessed on a cloud server. So then I could make a remote connection to it on the browser. Now, like I said at the beginning, there's even an easier way to use MCP servers locally now, which comes from Docker Desktop, which are not a sponsor of this video. I'm just showing you this feature where they have a catalog of tons of MCP servers that allow you to do one-click authentication. So, for something like GitHub, for example, I clicked on it, I authenticated with my token, and then once I did a one-click, I can go to clients. You can see right here they allow you to do a oneclick to things like claw desktop cursor Gemini CLI LM Studio which is basically a local language model repo where you can download open source models and you'll see here once I've connected them if we go back to cloud desktop and we click on search and tools we have this section that's called MCP docker where every single tool that's associated with both GitHub as well as duck.go go the web search browser I'll have access to right here and you can basically turn on or turn off any of these toggles. So if you don't want to overwhelm the AI with too many pieces of information or too many tools you can tell it here's five tools of the hundred focus on those only. And the best part about running something like this on your computer is this is free to use and it's free to host. As long as the associated service you're trying to use is also free then you're good to go. And it's literally as easy as me saying something like YouTube transcripts and then clicking on this plus button, then clicking add server. And now I'll have access to these two tools right here. If I click on this link, it'll say you can get a transcript and get video info. And this then again transcends to all kinds of tools because I don't just have to use it in cloud desktop. I can use it in cursor. So if I'm building something that needs a transcript to help me build it, I can go use that MCP server, bring it into cursor with like one click and then use it from there. And if we go back into Claude, you'll see right now if I go to MCP Docker and click on here, I should be able to search for transcript. And you can see right there I now have access to the YouTube get transcript function. Just a few months ago, this would take a few extra steps. Me showing you a JSON file, you doing it on your end, saying, \"Mark, this doesn't work.\" And then I would have to say no, it works on my end. Why does it not work on your end? And then we go in this endless loop, but now you can skip that pain and just get to the result. And last but not least, like I said at the start, I would show you a teaser of what it looks like to build your own server. So essentially, this is clawed code that I'm using in cursor cuz I like to use both side by side. And I ask it with a series of files here to create this special function which is not the lazy calculator. In this case, it is a function to help me with forecasting. So you can see here it's called the forecasting calculator MCP. And if we go back into Claude, the way it works is the following. If I send over a request to forecasting, okay, so I had sales over the last 5 months of $6, $9, $12, $15, $18, and $20. Can we use the forecasting calculator to help me forecast the next five months of sales based on this? So it will send it over. And the beauty of it is behind the scenes it's very deterministic. There's no AI or language models happening in my function. But because I am combining that function with something here using a language model, it can interpret the back and forth, invoke the function, interpret the results for me, and I can see the whole thought process. So if we take a peek here at the forecast data, you can see my original request. It comes up with a response right here from the actual function forecasted values. Then Claude takes that and interprets it and says, \"The linear regression forecast shows that sales are projected to continue growing at a $286 per month rate.\" And then it goes through some other stats and comes up with a full forecast. Now, initially, this took me a few hours to figure out. But the beauty of this is once you build one and it works, you can use that same code that AI can help you generate to build another one, to build four. So, for example, the lazy calculator took me an hour and a half to make it work. But this more advanced and actually useful forecasting calculator took me four or five minutes and three requests. And you can go one step further. Imagine building this but then using language models behind the scenes to act as your super tools that you can use anywhere. And because this is the USBC type of technology, if you hosted it on a cloud server, then you can really access it from your chat GBT, your cloud, and different services can benefit from one single tool. So hopefully that makes sense. I wanted to walk through conceptually the three different tiers I see around MCPs just so you can get more familiar and recognize them in the field. And hopefully it gets you excited to actually interact and use them for your day-to-day. If that was a helpful quick overview, let me know down in the comments below. And if you want to see things like this where I build MCP servers and show you exactly how to do it, then check out the first link in the description below for my early AI adopters community.",
          "summary": {
            "resumo_uma_frase": "Everyone knows about MCPs by now, but here's what nobody is talking about",
            "resumo": "Everyone knows about MCPs by now, but here's what nobody is talking about. 90% of people who know about it still aren't using it or aren't using it effectively. You've probably seen the demos, heard it's the future of AI agents, or even tried to set up one for yourself. But then you hit a wall of, \"Okay, how do I actually use this in my day-to-day workflow?\" After helping dozens of people actually implement MCPs, I've seen the same pattern over and over again. People get stuck between knowing that it exists and actually deriving value from it. So, in this video, I'm going to walk through when it makes sense to use MCPs for your specific use case and three different ways you can think about implementing them. By the end of the video, you'll have clarity on what MCP is, how to use it, and I'll even",
            "assunto_principal": "3 maneiras de usar servidores MCP agora mesmo (MODO FÁCIL)",
            "palavras_chave": [
              "about",
              "actually",
              "after",
              "again",
              "agents",
              "aren't",
              "before",
              "between",
              "building",
              "clarity",
              "day-to-day",
              "demos"
            ],
            "resumo_em_topicos": "- about\n- actually\n- after\n- again\n- agents\n- aren't\n- before\n- between",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "gUgwG0XPdkc",
          "title": "Por que você provavelmente não deveria ajustar seu modelo de IA",
          "url": "https://www.youtube.com/watch?v=gUgwG0XPdkc",
          "published": "2025-09-14T00:43:33.969007",
          "published_relative": "há 3 dias",
          "duration": "14:22",
          "date_published": "2025-09-13T14:30:05-07:00",
          "transcript_available": true,
          "transcript": "To fine-tune or not to fine-tune, that is the question. And right now, 90% of entrepreneurs, business owners, and companies are doing it wrong. They're spending tens of thousands of dollars to give AI a PhD in something it already has a master's degree in. Naturally, fine-tuning sounds very sophisticated, elegant, and fancy. But here's what most don't realize. Most of the time, you're teaching a model something it already knows, or worse, something you're going to have to unteach it in five to six months. It's like hiring a Shakespeare expert to teach Shakespeare how to write like Shakespeare. So, today, I'm going to walk you through my mental model and decision framework that I personally use to determine whether or not fine-tuning makes sense. And by the end of this video, you'll be able to better understand whether or not you need to focus on prompting, rag, or fine-tuning is actually worth the investment. Let's get into it. So when someone on a paid consult or someone in my community asks the question, should I fine-tune this model for XYZ use case, I'll usually point them toward this pyramid which myself and my agency prompt advisers put together. And the way it works is actually pretty simple. So at the very bottom, the foundation, the foundation of civilization is prompt engineering. And most times you can derive a lot of value and a lot of hidden value that you don't even know language models are aware of and then not have to do anything else. But if you feel like you've tried every single model and you need some additional context or a mini knowledge base of sorts to keep your prompts on track, then it could make sense to do what's called context dumping, which is taking a language model with a very large context window. Think of Gemini 2.5 Flash or Pro or GPT4.1 Mini or Nano that have a million context window. And if this is still not good enough, you have retrieval, augmented generation, and within rag, you have many layers of rag you can use before you actually have to ascend to the very top of fine-tuning. In most of the cases we see, 90% shouldn't be fine-tuned. And from everything myself and my team have seen, around 90% of cases usually don't need any form of fine-tuning. But there are some valid 10% use cases where whether it's security related or company IP related, it could make sense. Back in 2023 and early 2024, it made way more sense to fine-tune because language models themselves just weren't where they needed to be in a lot of areas. One of the most common areas was JSON or JavaScript object notation where outputting code or outputting payloads from a backend using a language model was so inconsistent that fine-tuning made sense. and training it on what that payload should look like, all the syntax it should adhere to over time made a lot of sense. But now you have things like structured output which makes sure this doesn't happen. And in general you have much smarter language models that are inherently and natively better at this. Then we had other things like tool use and code generation as well as reasoning where before we didn't have reasoning models but now we have things like 01 03 04 we have deepseeek we have different versions of deepseek so a lot of the core need to have to fine-tune are not there anymore and like I said at the beginning a lot of these models already inherently know a lot of the information that you're fine-tuning for. So when it comes to doing things like legal, medical, academic, writing in the style of dead authors or dead artists, a lot of this stuff already lives in the model itself. And one of the million ways you can test this is if you ask a language model, can you write this piece of information in the style of Shakespeare, Elon Musk, and Gordon Ramsay, and you combine them all together? it understands inherently in its corpus of training all of those different styles from all the snapshots of millions of pieces of text it's ingested over time. So in the same vein, I've met with multiple people who have fine-tuned or overfinetune a model for something that actually inherently exists in the language model training itself. Now you might look at this and say you have objections because your specific case makes perfect sense and it might. Now, one of the most common use cases that I do get is around brand voice where I ask the question, and I'll go to it here. I actually call it the brand voice evolution trap. Do you think that your brand voice will stay the exact same for the next 1, 2, 5 years? Because realistically, the way I speak now is different from the way I spoke a year ago, which is different from the way I spoke 5 years ago. Just take a look at my older YouTube videos. Actually, no, please don't. But the way I even articulate myself, the words I use, the analogies I refer to are a lot more basic than now where I've had more reps at actually speaking online and getting that feedback loop from you. In the same way, if you were to fine-tune, one of the most common use cases, I have 5,000 pages of me writing and reading. I want to now make this a fine-tuned model so that I can always just tell it write this essay or write this blog post and it will always sound like me. And again, I'll say for 6 months, maybe a year, this will be an awesome model. But what happens if you want to change your style? Do you want to go back and retrain? How are you going to retrain? Are you going to retrain with everything from before, and then now you add a stratified sample of the new version of you you're trying to be, or are you going to try to do some prompt engineering to that fine-tuned model to make it adapt to the new version of you you're trying to be in the future? If you're not on the technical side of things or less techsavvy, then you can trap yourself with a fine-tuned model where all the models are advancing. All of them would actually understand better over time if you uploaded two documents how to mimic your style. But now, because you've planted some roots, you're stuck and you've sunk time and money to make it work. And one example is if you look at the Twitter profile of any conglomerate company, any large company, you're going to see messaging 10 years ago is very different from messaging now where they're trying to adapt to the cool kids, the new lingo, the new jargon of the day. So imagine they ran their entire X or Twitter account using a language model trained on millions of tweets from before and now they're trying to adapt a new audience. This wouldn't work. Now, initially I've spoken to what doesn't make sense, but let me touch on what could make sense. So, one use case that makes perfect sense is if you are a SAS company or you run a SAS company that's kind of an LLM wrapper where your entire business has some bells and whistles on the product front, but at the end of the day, the foundation of your app is a language model. And a really good example of this are Vibe Code apps where now there's thousands of different versions of them. Many of them depend on cloud 4 or cloud 4 sonnet to actually generate the code. One thing I've heard of over time is some of these companies taking open- source models, training them on vibecoded code and frameworks and the best way to go from zero to 80% in under 3 minutes so that they have one their own moat, two their own like dependencies that they don't have to worry about claw tomorrow shutting down access, increasing their prices or even having a model misfire or degrade. And by nature of that, their product would degrade, but their customers don't care who to blame. It would be them to blame. And I've also heard this for voice agent companies as well, where initially before voice agent technology advanced to the point we're at today, they took models and fine-tuned them to be a lot better at having conversations that we have colloquially on the phone versus a text generation through something like chatbt or claude. And another use case that makes perfect sense is volume-based arbitrage where you maybe you have so much inference. You're sending so many messages in the millions that it doesn't make sense even with the cheaper closed source models to run it versus you having your own open-source model that you host on a server. And once you do that math, it adds up. The one main thing I'd note here is depending on how your business works, if you need instant responses from language models, then this would make even more sense. But let's say you could actually ceue up these responses or have them sent the next day. You could look into something like a bulk API where you send 50 100 messages at once and then you get a nice 40 to 50% discount off of your cost. And the next one is a very common one that's justified where there's compliance. You have GPDR, you have HIPPA, you have ISO, whatever it is. You could be in an industry where even hosting a language model on a cloud on something like AWS Bedrock or Azure still might not be good enough. If that is the case, then it could make sense to fine-tune something like a Mistl or a Llama for that specific use case so that you have full control and you have a lack of security audits. One thing I would say on this though is that these companies like AWS Bedrock do allow and do work with some of the largest healthcare providers in the world. So if you are in a gray area where you have clients, you have stakeholders, you have colleagues that are concerned, they literally publish a list of every single company they work with. And then the last one here relates to my first point where you could build some form of a moat. So let's say you have tons of proprietary information, tons of proprietary data that isn't readily available online. The language model companies haven't managed to acquire or purchase it or maybe it's impossible to acquire without your specific mode. So in that case, if you have something genuinely proprietary and it doesn't make sense to do rag on those 50,000 documents because there's something stylistically different, there's information that is hidden from the world or at least hidden from the language model that's there that you wouldn't find anywhere else, then yes, it could make sense. One more category where you could use fine-tuning is where you have static information that can't change. A good example of this, like exhibit A, was someone last week reached out to our agency asking whether or not we could help build a service or a SAS platform to enable someone to upload the messages, books, anything from a loved one that's deceased and fine-tune a model to sound like them. In this case, there's not going to be any form of brand new information. So, given that, you could fine-tune a model because that voice should not change. And moving on to regulatory. If you are in the legal space and there's some form of legal adherence or some form of verbiage or statutes that you have to adhere to every single time or a specific style, then it could make sense. And for the last one, I referenced this before. If by some miracle there's a creator or artist that is completely unknown to the world where they don't exist in the language model data, I would triple quadruple check in multiple ways. not just by their name but also by the name of their works by a copy paste of a snippet of their works and asking it do you know where this is from and kind of pulling on that thread because sometimes due to copyright issues these language model companies have to keep it very well hidden. So once you verify that it's genuinely not there then this could also be a good application. So if you are assessing this either for yourself, for a client, for your team, so if you want something structured like a checklist to audit whether or not one, it makes sense to fine-tune and two, whether it still makes sense to fine-tune if you've already done it, this could be a good set of questions. I'll make this available to you in the second link in the description below and it will include this really nice mermaid diagram that I'll show you right after. So first thing is relevance check. Has our business model shifted? Have customer expectations changed? Are base models now doing X natively? So again, before models couldn't do something like JSON very well, but now they do. So have models changed where your fine-tuned model that you're paying inference for to host the open- source models and maintaining those models and adding to them, is it still worth it? And then if you were to put the fine-tuned model head-to-head with the newest base model that is affordable at scale, would the fine-tune model still outperform on that use case that you used to originally justify it? On the economic review side, this is one of the most important things to look at, which is let's say we have Claude 6 and Claude 6 is brilliant but also very cheap. In that case, would it still make sense for you to fine-tune? And again, if it's related to compliance, then probably the answer will still be yes. But in many other cases, it won't. And the last part, if you ask yourself very honestly, one year, two years down the line, let's say you trained a model or fine-tune a model on company data and it's super proprietary, you have to ask yourself with everything that's inevitably going to advance in the next couple years, is it still such a competitive advantage that it's worth maintaining and pursuing? And then if not, it makes sense to run the numbers as to whether or not it would make more sense to switch to whatever language model API we have in the future. And if you're newer to the concept of fine-tuning, I actually made a video about fine-tuning a while ago. That was actually pretty decent. And it shows you exactly how to do it at least at the lower scale using OpenAI. But other providers, if you were interested, include Azure OpenAI. You have AWS, like I said, AWS Bedrock, which has a partnership with Claude. And I think Amazon is invested in Anthropic as well. And then you have hugging face which is the mecca of open source models where you'll find all kinds that are quantized meaning they've been compressed. So if you're looking for a certain type of model with a certain type of size odds are you can find it there. And then you have something like together AI that also allows you to host a model you can own the IP of that fine-tuned model and then you can run inference and connect to it. And I have this mermaid diagram to walk you through a mental model of the steps here of whether or not you're processing this many tokens per month. If so, do you have specific requirements like regulatory? If yes, can you dedicate two plus engineers for 6 weeks to go from zero to a fine-tuned model with your kind of data? If so, you keep following the breadcrumbs here. And this is not a rule in the space. Everything that you'll see here, this is just something that's worked for us after seeing tons of different cases with hundreds of businesses. And that's pretty much it. I just want to touch on this to save you potentially a lot of time, a lot of anguish and suffering, and most importantly, money. If this helped you in any way, shape, or form, let me know down in the comments below and give a like to the video. It helps me, helps the channel, helps everything. And if you want to see even more topics related to prompt engineering, fine-tuning, and pretty much anything around Genovi and actual implementation of Genai in businesses, then check out my early AI adopters community. The first link in the description below. I'll see you the next one.",
          "summary": {
            "resumo_uma_frase": "To fine-tune or not to fine-tune, that is the question",
            "resumo": "To fine-tune or not to fine-tune, that is the question. And right now, 90% of entrepreneurs, business owners, and companies are doing it wrong. They're spending tens of thousands of dollars to give AI a PhD in something it already has a master's degree in. Naturally, fine-tuning sounds very sophisticated, elegant, and fancy. But here's what most don't realize. Most of the time, you're teaching a model something it already knows, or worse, something you're going to have to unteach it in five to six months. It's like hiring a Shakespeare expert to teach Shakespeare how to write like Shakespeare. So, today, I'm going to walk you through my mental model and decision framework that I personally use to determine whether or not fine-tuning makes sense. And by the end of this video, you'll be able to better understand whether or not you need to focus on prompting, rag, or",
            "assunto_principal": "Por que você provavelmente não deveria ajustar seu modelo de IA",
            "palavras_chave": [
              "actually",
              "advisers",
              "agency",
              "already",
              "better",
              "business",
              "case",
              "community",
              "companies",
              "consult",
              "decision",
              "degree"
            ],
            "resumo_em_topicos": "- actually\n- advisers\n- agency\n- already\n- better\n- business\n- case\n- community",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "VuULRih34PU",
          "title": "I Tested 4 AI Browsers So You Don't Have To",
          "url": "https://www.youtube.com/watch?v=VuULRih34PU",
          "published": "2025-09-14T00:43:33.969024",
          "published_relative": "há 3 dias",
          "duration": "20:17",
          "date_published": "2025-09-12T18:15:07-07:00",
          "transcript_available": true,
          "transcript": "AI browsers are having their iPhone moment right now, and each one of them is claiming to revolutionize the way we browse. But while everyone's arguing about which one is the best, they're missing the bigger question. Are any of them actually worth using? I tested these browsers across a few practical scenarios to determine whether or not they're any good. And even if you decide that any of them are worth using, no one's teaching you how to prompt them correctly because the way you prompt an AI browser or an agentic browser is very different from the way you prompt something like chatbt. So today I'm going to walk through four of the more popular browsers and more importantly how to actually prompt them and use prompt templates to unlock their true potential. And these are templates that you can just copy, paste, and tailor for your personal use. If you watch till the end of this video, you're going to learn three things. One, whether AI browsers are worth your time at all. And two, if they are worth your time, which one fits the specific needs that you have? And three, the prompting frameworks that separate those who just burn their money in subscriptions versus actually derive value from them. Let's get into it. All right, so today I'm going to be reviewing these four popular agentic browsers. First, we're going to take a look at Comet from Perplexity, then GenSpark browser from Genspark, and then Fu as well as Dia browser. And before we get into the nitty-gritty, let me show you a quick tour of each browser. So this is what Perplexity Comment looks like. And you can grab it from someone who has a private invite who can bring you on for free for the cost of subscription, or you'd have to pay $200 a month, which is pretty steep. In terms of the interface itself, it looks very similar to the normal Perplexity where you have three tiers of search right here. Then you have all the same tools as well as the ability to invite others once you get access as well as this nice and handy dandy assistant which if you click on pops up as a co-pilot. And that's one element that all of these browsers share in one way or another. But you can basically have something on screen while this looks at what's on screen and then gives you commentary or does research. So theoretically you could pull up an edit in workflow and then click on the assistant on the right hand side then ask something like can you look at the workflow on screen and explain to me what's happening and what a open AI embedding is and then if we send that over it should be able to now control the screen and you'll know that because it'll be a pulsating blue wave on the screen. It'll be able to look at what the JSON looks like behind the scenes and then actually interact with it. As you can see here, it's pulsating because it's actually interacting with every element on screen to be able to click on a specific node and come to the conclusion of what we're looking at. And the next one is the Gen Spark browser, which just came out with a brand new feature that does differentiated compared to the competitors. So, if you haven't used GenSpark before, essentially it is a super app that lets you do all kinds of things like create AI docs, create AI vibecoded apps, design, even something like an Opus clip as well as AI podcast and chat and all kinds of other functionality. And essentially, the browser just lets you access GenSpark on your device. But the newer feature that they've added is the ability here to have ondevice free AI. So, if you open this, you could theoretically have a local language model on your computer. And you can see right here I have GBTOSS, OpenAI's open source model on my local computer that I can just tap into and use and then I can use it for the Agentic browsing behind the scenes because one major theme that you're going to notice in all of these browsers is that when it comes to security, they don't do a great job. So the ability with this one to have an offline mode definitely gives it a bit of an advantage. And one more thing specifically about this browser is you can connect tools and MCP servers to it. So one major advantage is that they have a mini MCP marketplace. So as you go through you can see right here we have HackerNews MCP and all kinds of community generated MCP servers. The next one is the FU browser and this one is really different in terms of the look and feel. So if I go into a brand new tab here, it uses a credit system and it has two main modes. One is deep thinking and one is deep action. And then this little at sign allows you to compare multiple tabs. Now in this browser they make the at simple intentional to tell you that if you have one or two tabs or multiple tabs open you could do at to say go and read these tabs and do xyz task and then as you type it will try to do some autocorrect or auto prompting for you. So if I say something like go research the best ways to use vibe coding apps is going to try to find a way to actually optimize your search or prompt so you can make it a bit better. So you can see here compare different vibe coding apps and recommend the most efficient ones. But one very funky thing about the user interface here is that while you can open tabs here, you can have different chat sessions stored on the right hand side of your browser. So if I click on this right here, you can also initiate having a deep thinking conversation or a deep action conversation where it comes up with a very comprehensive plan. Takes anywhere from 5 to 20 minutes to try to execute this agentic task. theoretically sounds awesome. In practice, not the most reliable. And the last one is DIA browser, which is very clean to look at, but definitely lacks in terms of the agentic capabilities. It does have a very minimalistic look and feel. And if you go to something like skills and you go to browse skills, you'll see right here there's a few different slash commands that you can add. So if you want to click on fact checker, you can click on try and dia or dia browser, then click on add skill. Then once you add that skill, you can do a slash command to actually call that specific skill. So essentially when you call /f fact check, it will run this prompt behind the scenes. And you can see now that I installed it, I can do slashf fact check and then it could be regarding something specific. Let's say fact check this site. And behind the scenes, it will execute this underlying prompt that you can edit if you want. So it's a very minimalistic browser and you can have this little co-pilot at the right hand side as well that can see what's on screen. But again, in terms of actually executing things or doing things on your behalf, it's lacking compared to the others. So that's a quick glance at all of them. So now, in terms of testing it, instead of using the prompts and using them live, it'll be a bit tricky to show because a lot of these browsers use a lot of RAM, and at some point, my computer was heating up. Now, mind you, I had four running at once, so that's a big reason why. But some of these browsers are less computationally efficient than others. For example, I have a decent computer, but when I used the FU browser at the same time as the DIA browser, my computer had a little bit of a seizure. So, I'm going to walk you through prompts that I tested, and I'll walk through the results of that test. So, this was the first prompt where I just asked, \"Go to TechCrunch, The Verge, and RS Technica. On each site, navigate their AI or technology section, then find and read their most recent article about AI agents or autonomous AI.\" And then I said extract key announcements, company names mentioned, and predicted timelines. then search for each mentioned company's website, navigate to their about or product pages, and verify the claims made in the articles. And the core output is to create a fact check summary comparing what journalists are saying versus what companies are actually claiming on their own sites. So, Perplexi did a decent job where it went and if you go on the steps here, it navigated just using its normal browser capabilities went through eventually got to the point where it actually started looking at the sites and inspecting them using its agent. And it did that for different parts as well. And you can usually walk through while it's actually doing the research and navigate and see what it did on that page. In terms of agentic capability though, it didn't actually have to go through and scroll manually through each page and copy paste information because Perplexi is naturally really good at research. On GenSpark's side, it went and actually used Playright behind the scenes, which is a browser automation tool. And then you can actually spy by clicking on view exactly what it did, what it clicked, which elements of the page it clicked, and what information it derived from those pages. And you can see also the execution results throughout. And you can see how much work it did, and what web pages it saw. So in a way, you're getting a perspective view, a POV of exactly what it saw, what it did, and what information it collected. And you can see that for each version of its automation that it ran behind the scenes. So you could get a really good way to audit exactly what it saw and most importantly what it derived from what it saw. All right. And on the FU browser, it did a lot of things, but it didn't necessarily get to the result we were looking for. So I'm going to zoom in here because I can't seem to zoom any other way. But you can see I have the same prompt. In this case, it comes up with a deep action plan. It comes up with a process of the browser agent running things in parallel. So you can see here, step one, navigate to TechCrunch website. Step two, find and click on AI or technology section. Step three, search for the most recent article. Then it goes through, executes the browser agent. You can see how many steps are going on here. And one thing about this browser is it uses a credit system. So every single time you run something using deep action, deep thinking or both, it is taking away from your wallet. And after doing all that research and the browsing and it took around 10 minutes, it comes to this task failed. So unfortunately this didn't do a good job at actually getting to the end point. In terms of dia browser you could see it said I can't directly browse the web or navigate websites in real time. However I can help you with the workflow to accomplish this and if you provide links or articles I can analyze and fact check them for you. So this one is a lot more you doing the work versus it doing the work. All right for the next task I asked essentially open two tabs. in one tab, open Stripe, in the other one, open Square, and basically compare their pricing. And I give it quite a bit of information on how to do that comparison. Now, the main thing I wanted to see here is whether or not it could actually physically open two tabs, then analyze them. In Perplexia's case, it doesn't actually physically open the tabs. It just does the analysis as it normally would. So, it does give you the information. It just didn't actually physically agentically open those two tabs. Now, does that matter? No. It's more so just gauging whether or not the physical nature of the browser is actually everything that they say it is. On the GenSpark side, it did actually open both of those tabs and then do the comparison between those tabs and report back. You can't see it here just because I closed the tabs after it completed, but it was able to physically do that action and come back with the final verdict and comparison. On the Fu browser side, it did actually work and it came back with more than what we asked for. So you can see here it came up with a plan once again very detailed plan and obviously this is burning some tokens. It comes with a comparison after doing the sideby-side compare and it comes up with data and then if you click on either one of these stripe data or square data you get something like this where it walks through its findings in detail which is cool. I would just say it didn't necessarily need to have to go through 22 different steps to get to this specific result. And then the DIA browser did come back with a pretty looking sticky note table with the information we're looking for, but nothing else beyond that, which in this case is fine. But again, it wasn't able to open those two tabs physically and audit them side by side. Now, for the next experiment, I made it very easy where I just asked it to analyze two PDFs that I already loaded on screen. So if we go to comet first from perplexity, you can see I opened this GPD5 system card which if you're not familiar essentially allows you to understand exactly how these models work, how you should prompt them, etc. And then I also open claude 4 opus and sonnet. So my goal was can you compare both of these PDFs, read through them and then come back with some results on what the difference is. So in perplexity you can do an at symbol where you can say at this tab and then at this tab go and research and compare them. And you can see here it's referencing those PDFs and only those PDFs. So if we go on the sideby-side compare, comes back does a really strong job. And most importantly, it did it in under a minute, which to me is really important is that we don't spend five business days to wait for a result that could take a minute or two. So in Gen Spark's case, I couldn't find a way to do an at symbol to look at two specific tabs I already have open. So when I ask it to compare both system cards, it's physically attempting to go and manually search both of their system card addresses. Then look at the tools there. So you can see here it's manually actually searching both, which I'm sure it'll come back with a response. You can already see it's summarizing the large documents, but it's not able to actually use both tabs we physically have open and use them in context. Now, in Fallu's case, I can compare both of these tabs here. So you can see the first PDF and the second one I have on screen. It does some thinking and then it goes through. But all of a sudden here we go into espanol even though I didn't even ask it or speak to it in espanol. So very interesting that it went that route. And it's not the first time I've ever seen this behavior from an agentic tool. Once in a while, Genspark will also go full espanol even though I have no settings to tell it to do that. And finally for the DIA browser, it does do a decent job of comparing both. It even has a nice little UI to show the comparison. It does a proper back and forth. And this one I would say is one of its best use cases. I've also tried it on three or four tabs and was able to work on those as well. And for the last prompt, I wanted to push each one of these AI browsers to do something truly agentic. So I asked it to go to upwork.com, log into my account, then navigate to the post a job section, then create a new job posting with the following details, but to not submit it. So I didn't want it to actually click post. And then I told it we're looking for a web flow developer for prompt advisor to make our site that much better and that much easier to read which by the way we are. So if you are a web flow developer please uh email me and I'd love to talk to you. And then as you go down here we go through the current situation the skills required and then the screening questions. So I wanted to set up everything from the posting to the screening questions and go all the way to the final stage. So, Perplexity Comment, if we go to the steps here, we can finally see it physically interacting with the pages. So, once I say I'm logged in, it takes control. And if I go on the steps here, you can see it actually physically navigated to Upwork and executed all of these different tasks. And I can tell you it got all the way to the very end. The post itself did look very AI sloppy with the little rocket emoji and stuff, but it did work. The only downside about comment is that if you're not watching it live, you can't actually physically see it do the work. All right, for GenSpark, it wanted me to make things a lot more concise. It couldn't take that long of a prompt. So, I compressed the prompt. I provided it. It looks very similar. I just removed some of the details. And then it goes and uses playright once again, but in this case, it gets a Cloudflare error warning where it's detecting that it's an AI agent or an AI browser. It's not letting it go to the next step. In this case, it tries to get past it, but it then defaults to giving us everything we need to do it ourselves. And for the Flu browser, it took around 19 minutes and 53 seconds, so call it 20 minutes. And it came up with a very comprehensive plan. And for this one, I would say it did a really good job. It went to the final stage. The post doesn't look too sloppy. It just has some markdown, which is fine, but overall did a really decent job and navigated through each part. Now, did it choose the best rating for the hourly rate? I I don't necessarily have the full logic here. I could probably read through all the steps as to why it decided this would be a good amount. So, it says here, budget 50 to 100 an hour. But in terms of passing the test, it was able to execute it. And last but not least, DIA browser did struggle a bit where it says I'm unable to directly access websites, log into accounts, or perform actions such as creating job postings for you. So in this one again, Dia browser seems to be a great co-pilot but not the best agentic browser. So now that you have a decent idea of exactly what it looks like to use each browser and where each one has an advantage over others. If we look at prompt engineering, this is something that spans across all of them. So in general when it comes to research tasks this is the structure of a prompt you want to send where you have a role a topic trusted sources and then you tell it first gather information about X then analyze Y then synthesize by doing XYZ and then if you wanted to fact check then you could say create three verification questions about your findings and answer them independently and if there's a specific format you want it to deliver in you could just say at the very bottom deliver results as format with specific requires comments. And by the way, you don't have to screenshot this. I'm going to make this available to you in the second link in the description below. Now, for the browsers that can do a multi-tab comparison, you could do this where you do at tab one, then at tab 2, then you could say compare on specific elements across tab one and tab two. And in some of these browsers like Dia browser, you could do three or four tabs. So then you'd say extract data point one from tab one, extract data point two from tab two, and do the differences and similarities in this way. And again, always tell it at the very end what the deliverable should be. For the autonomous actions, ideally, you want to give it a goal very similar to how you'd give it to a normal agent in an agent framework. So, you want to say what the goal is and what websites you need to navigate to to achieve again the outcome. It's good to restate it. And if you have requirements, then mention them here and then say stop before final action. In this case, don't submit or don't send the email and await verification. So, in a way, you create a human in the loop setting for the browser. And then similar to the very last one I showed you, the Upwork one, this is the general template where again you have some objective or end goal. Then you walk through what the different phases are. And this might help a browser like Fu or Comet or GenSpark to be able to take each one of these steps and do it in one chunk at a time. And the more deliberate you are about how you format the bottom, the better because there's always recency bias with prompt engineering. So if you say this is the format, this is the length, this is the key emphasis, it'll be a good way to reinforce. And if you don't want to send it in one shot, then what you can do is use progressive prompting sequences where you could say stage one, I give you the broad context. What are the main topics and trends in XYZ industry? And then the next one could be focus research where focus specifically on narrow topic from this time period and then according to XYZ source, what are the key findings? And then the last one again is just saying what the specific output is and now it has the context of the entire conversation up until now that it can draw on and use as a part of it. So if we were to just look at this as a general process and zoom in, then you'd have to choose your AI browser first. Is it a simple question? If yes, then just do a basic research template prompt. If it needs multiple websites, then maybe do a multi-tab comparison. And then if it requires clicking or navigation or any form of spectrum of agency, then use the autonomous actions prompt template. But if you find that that one mega shot prompt is not doing it for you and it keeps hallucinating or not getting to the end result, then you can break it up into more digestible and manageable chunks. Then use a progressive sequence or prompting sequence. And then very importantly, I'm going to say if you have anything involving sensitive data, I've had disasters happen in one of these browsers where I open up my Gmail and I told, can you just like remove certain emails that have XYZ, but can you place the other ones in the associated folders that I list for you? What it did is it took everything out of the folders. It marked it all as unread and now I had from an inbox of five emails to 5,000 emails that I had to get my VA to help me sift through. Once again, if you want to do research or comparisons, these are awesome vectors. But for now, until security is actually cared about versus just the power and fluency of these tools, I wouldn't make the trade-off. And as a too long don't read, I have a summary of each one of these pros and cons of each browser where it comes to the price versus the functionality as well as security as well. So instead of me extending this video and going through them step by step, I'm going to make this available along with the prompts I showed you as well, so you can make a very informed decision and when you make the decision, you'll know exactly how to prompt it to derive all the juice you can from that specific tool. If that was helpful for you and saved you some time, please let me know down in the comments below. helps the video, helps me, helps the channel. And if you like me synthesizing this kind of information in a way that's easily digestible, I do this every single day, 365, 24/7. Well, not 24/7, but a lot of the time in my early AI adopters community. So, check out the first link in the description below and I'll see you in",
          "summary": {
            "resumo_uma_frase": "AI browsers are having their iPhone moment right now, and each one of them is claiming to revolutionize the way we browse",
            "resumo": "AI browsers are having their iPhone moment right now, and each one of them is claiming to revolutionize the way we browse. But while everyone's arguing about which one is the best, they're missing the bigger question. Are any of them actually worth using? I tested these browsers across a few practical scenarios to determine whether or not they're any good. And even if you decide that any of them are worth using, no one's teaching you how to prompt them correctly because the way you prompt an AI browser or an agentic browser is very different from the way you prompt something like chatbt. So today I'm going to walk through four of the more popular browsers and more importantly how to actually prompt them and use prompt templates to unlock their true potential. And these are templates that you can just copy, paste, and tailor for your personal",
            "assunto_principal": "I Tested 4 AI Browsers So You Don't Have To",
            "palavras_chave": [
              "about",
              "across",
              "actually",
              "agentic",
              "arguing",
              "because",
              "best",
              "bigger",
              "browse",
              "browser",
              "browsers",
              "chatbt"
            ],
            "resumo_em_topicos": "- about\n- across\n- actually\n- agentic\n- arguing\n- because\n- best\n- bigger",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "jt9ecYyCTi4",
          "title": "O Agente Replit 3 é MUITO mais inteligente do que você pensa",
          "url": "https://www.youtube.com/watch?v=jt9ecYyCTi4",
          "published": "2025-09-13T00:43:33.969039",
          "published_relative": "há 4 dias",
          "duration": "10:06",
          "date_published": "2025-09-11T18:15:09-07:00",
          "transcript_available": true,
          "transcript": "Replet Agent 3 is here, and some of the updates are genuinely helpful. If you're someone who finds Cloud Code way too intimidating, but the other apps way too rudimentary for what you're trying to build, this new update will sit right in between of those two worlds. Instead of just running for minutes, Replet Agent can run for up to hours, which means for a variety of apps, you can actually one-shot and get pretty far. But the part that really caught my eye was the fact that it can finally test its own work. So when it builds your app, it actually goes and checks your requirements, goes through and enters test data throughout the whole thing to check whether or not it actually accomplished the task. I spent a few hours with it yesterday, and I want to show you exactly what I built, how I built it, and a lot of these new features along the way. Let's get into it. All right, so when you log into Replet, you'll see something like this where it says at the very bottom, introducing agent 3, autonomy for all, and it goes through that very feature I mentioned, which is app testing. and they've added some preliminary agents and automations. So theoretically for a handful of apps that they're going to continually be adding to, you can create micro automations, whether it's to Slack or Notion or even something like GitHub. And while it says it can run for up to 200 minutes, realistically, usually it's anywhere between 30 and 50 minutes from what I was testing. And the cool part is you can actually set it running and if you have the mobile app, it will send you a ding when it's actually finished. But if you click on the agents and automations, you'll see these are example triggers from Slack, Telegram, and then timebased ones. And as you build, you'll be able to choose other ones along the way. So, this is the app I put together. And it took probably a few prompts to get running, but it did build 80 to 90% of it on its own. And it wasn't some to-do list app. It was actually something that is a follow-up to my last video where I walked you through how to use model system cards from these model providers to tell you exactly what changed when models change and how you can actually prompt them and which prompts work better with ones versus others. So, the goal of this is that you can actually upload different files, whether it be GPT5 versus GPT4. And while this runs, we're going to take a look at the prompt, but basically what it's doing behind the scenes is using GP5 to analyze both model cards and then creating a series of different comparison screens. And if we scroll till the very bottom of the screen and go to a part that's called prompt translator, this will allow you to put a sample prompt and have it output a version of that prompt that's optimized for the brand new model you're taking a look at. So, if we read through the prompt here, I gave it one that's more robust. You can give something that's just a short paragraph. It doesn't necessarily have to be fully fleshed out like mine is. I used AI to help me make mine a bit more refined, but you can see it says, \"Build a web application called model card comparator. The app should be designed with a clean, minimalistic, and light UI theme. The application must have the following core features. So here I ask for the dual PDF upload and then the AI powered comparison analysis. Then I go through some requirements there and then a sample prompt evolution. So generate a section that shows sample prompts to illustrate the evolution between those two models. For the previous model, display three to five sample prompts that would work well according to its capabilities. For the latest model, show how those same prompts could be improved or what new types of prompts are now possible. And last but not least, like I showed you, I ask for the interactive prompt playground where I say create a final interactive section called prompt translator. Provide a text input box where a user can enter a prompt they would have used for the older model. And then upon submission, it goes and actually checks the latest requirements and outputs the new version of that same prompt. So at the very beginning, it came up with a plan and started planning all the subtasks it would create and then it asked me for my open AI key. So after the very first prompt and around 20 30 minutes passed, it went through checked its work and came to the conclusion that it didn't have a way to test the upload because it didn't have a PDF at its disposal. So it asked me to upload two PDFs to the codebase, which I'll show you how I did so it could actually reference them in its testing. So in here, if you go to the very right hand side and click on open files, you'll see here that I threw in GPT5 and GPT4.5 as PDF files. And then I told it I not only uploaded the files, I clicked on compare analysis and I got this error. So I also tested it out to give it a bit more of a feedback loop. 13 minutes later, it came up with a series of actions, fixed a lot of things. And you can see here if I scroll down, it not only created a new plan, it actually used its testing capability. So if you watch the replay here and we can blow this up a little bit, you can see here that it goes from a blank screen to actually uploading the very first file and then it uploads the next file. You can see now it's choosing the file from its reference and now it should click on start comparison analysis. It waits and audits whether or not it actually comes to result, which is usually the most annoying part. So here, right here, you can see that it came to this analysis failed. So instead of me having to figure that out myself and tell it that there was an issue, it realizes that right here, I've hit a snag. And this is one example of really useful productization of this technology. After it realizes it messed up and then restarts the application, it works on the file upload issue. It tells me what the bug is, so it's able to upload the PDFs, but nothing actually gets submitted to the language model GPT5 behind the scenes. Then after diagnosis, it goes through, creates new logic, restarts the application, tests it again the second time, and then if you keep scrolling, it sees that it made progress, it process the files, and at the very end, it says it's worked, and then it tests it one more time. And you can see here it finally uploads both. It clicks analyzing. And if we scroll ahead, you'll see this takes a while, cuz it does take a while. And then it realizes that it has results. So, as it scrolls down the page, it'll see that it has analysis results. It has the comparison, and most importantly, it has some example output prompts. It then tells me it's working perfectly, which to me, as someone who is a traumatized victim of Vibe Code apps, I didn't I took it for granted. Basically, I then tested it out and it did work. So, if I scroll down, you can see these are the analysis results. It references the files and if I zoom in just a little bit more, it walks through the model architecture. It differentiates between both models. It goes through performance metrics, hallucination rates, new capabilities of the new model. So, it puts the new model in green and the older model in blue. And then it goes through some before and after prompts. The after prompts being what it would look like if you took this specific prompt and adopted it or adapted it rather for GPT5. And then at the very bottom, like we said, we have this prompt translator where I could put a very vague prompt like build me a PRD for a robot. Or in this case, let's change it up. I want you to create a prompt to act as a prompt engineer so that I can write really nice essays about how much I love vibe coding apps. Okay. Okay, if we send this to optimize prompt behind the scenes, this should take the original one, send it over, use the logic and understanding of the brand new model with the model card or the model system card and come out with a result. And you can see here we get a prompt optimized notification. Now, this is one mega prompt. I would probably give it feedback on exactly to make it more concise, but it even goes through the key improvements it's made. And to be honest with you, I built something like this in the past. I couldn't use a vibe coding app because it just couldn't get me to that 100%. So, I use a lot of Python. So, this does work pretty well and it's only my third or fourth real prompt that I've given it. So, in broad strokes, this is how it works at a high level, but there are a few more bells and whistles to be aware of. So, at the very bottom here, you'll take this for granted very easily, but this is where you tell it basically, I want you to do app testing every single time. Now, when you use app testing, naturally you're going to be using more tokens because it's using image recognition, some autonomous agent to actually look at the screen, click around. But if you do something like max autonomy and high power, this is where you're really pushing the language model to its limits, but also your credits to its limits as well. And then right here, you can also enable or disable web search as well as image generation if that's a part of your app that you care for. Now, since it consumes more credits and takes way more time, you want to be more sure about your plan, especially if you want to make changes that you're not sure about. So, make sure that you take advantage of the fact that it does have a plan mode. So, you can ask questions, go back and forth, and only when you're sure and you're aligned on the plan should you execute it. And one more thing, a lot of times it does have that replet look and feel UI. you can start to use what are called themes which are now in beta where you can actually specify exactly what colors, what fonts and what components should look like from top to bottom. So theoretically you could name it a specific theme and always apply it to this specific app or any app you create and you could use AI obviously to help you come up with really nice complimentary colors. So in terms of where and when do I think you should use replet agent. Now, like I said, if it's a fully hands-on end-to-end project where you want to be involved in the nitty-gritty of the code, then using something like Claude Code is awesome. If you want to create a very quick and dirty app that spins up in minutes, maybe a more rudimentary app is the best way to go. But if you wanted that nice balance between something that is functional, something that has a database, and that's something that can work pretty autonomously over 20 to 30 minutes, maybe sometimes up to an hour and a bit, especially if you're not technical by nature, and you do want to be able to have this co-pilot at your side that you go back and forth with. But that being said, take it for a spin yourself. I think the addition of the AI agent that goes through and checks your work for you is basically a nice extension of typically what you would have in MCP servers like Playright and Puppeteer. So, it's a really good way for you to reduce that feedback loop and spaghetti code rage that a lot of us have. If you want $10 of credit to use towards Replet Agent, then check out my referral link down in the description below and hopefully that helps you get started. I'll see you the next one.",
          "summary": {
            "resumo_uma_frase": "Replet Agent 3 is here, and some of the updates are genuinely helpful",
            "resumo": "Replet Agent 3 is here, and some of the updates are genuinely helpful. If you're someone who finds Cloud Code way too intimidating, but the other apps way too rudimentary for what you're trying to build, this new update will sit right in between of those two worlds. Instead of just running for minutes, Replet Agent can run for up to hours, which means for a variety of apps, you can actually one-shot and get pretty far. But the part that really caught my eye was the fact that it can finally test its own work. So when it builds your app, it actually goes and checks your requirements, goes through and enters test data throughout the whole thing to check whether or not it actually accomplished the task. I spent a few hours with it yesterday, and I want to show you exactly what I built, how I built",
            "assunto_principal": "O Agente Replit 3 é MUITO mais inteligente do que você pensa",
            "palavras_chave": [
              "accomplished",
              "actually",
              "agent",
              "along",
              "apps",
              "autonomy",
              "between",
              "bottom",
              "build",
              "builds",
              "built",
              "caught"
            ],
            "resumo_em_topicos": "- accomplished\n- actually\n- agent\n- along\n- apps\n- autonomy\n- between\n- bottom",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "h0rmHYIRXjM",
          "title": "Como dominar QUALQUER novo modelo de IA em 10 minutos (ILEGAL)",
          "url": "https://www.youtube.com/watch?v=h0rmHYIRXjM",
          "published": "2025-09-12T00:43:33.969053",
          "published_relative": "há 5 dias",
          "duration": "09:17",
          "date_published": "2025-09-11T13:30:05-07:00",
          "transcript_available": true,
          "transcript": "Every couple weeks, we get a brand new AI model. And every single time, I watch as people panic. They scramble through tutorials, join Discord servers just to hop on and ask what's changed, and waste hours trying to hack prompts, hoping that something sticks. Meanwhile, the answer is practically always sitting right in front of you in a document that almost everyone ignores and creators barely mention. And I think the reason they barely mention it is to make sure that you're dependent on them to predigest the information and spoon feed it to you. These documents are called model cards. Every AI company publishes them, but nobody tells you how to read them. But in this video, I'm going to share the 10-minute secret that I use every single time to stay on top of my prompting game. No more guessing, no more broken prompts, and no more starting from scratch. If you watch till the end, then you'll never be confused or overwhelmed when a brand new model comes out ever again. Let's jump in. All right. So, I'll show you what these model cards look like in a second, but at a 50,000 ft view, this is what a model card is. Imagine you're playing a video game. And in this video game, you have characters, each of which have stats, whether it's NBA 2K, FIFA, or any game that comes to mind where there are stats for characters. And in this case, as each company rolls out a brand new model, each model has a set of stats and benchmarks and performance metrics that a lot of people care about. But most importantly, they speak to what the special moves, the special powers, and the special limitations of each and every model is. This is an example of an actual model and/or system card from OpenAI about GPT5. And on the table of contents, it goes through everything from an introduction to the model to how it was trained as well as the model data as well as refusals, sick fancy, jailbreaks, prompt injections, everything you need to know. And a lot of times people just guess or go by what they see on X to accumulate their knowledge about a particular model. But in this case everything is written here. You just have to go through it. And luckily we have language models at our disposal. So you don't necessarily have to read this. Now I do because I don't have a life but you have a life. So let's find a way to make this work for you. So I said this would be a 10-minute workflow. And I'm not going to lie. Basically how it works is all you do is you grab model cards. Now, in this case, it's helpful if you're using the same language model provider and you're going to the newer model in that family. So, example, let's say you're using Claude 3.7 Sonnet and now you're using Claude for Sonnet. One of the many things that you could do is download both model cards and then ask a language model to compare them. And obviously to do that, it's helpful to have a good prompt. And don't worry, I got your back on that. Once you paste said prompt into whatever language model you want, claude, chat, GBT, open source, what have you, you basically use this to do a master comparison prompt where you see the pros and cons of each and ideally you go back and forth on your particular prompts and basically ask here's my prompt that I used with 3.7 sonnet. How should I change it ever so slightly for claw for sonnet? And you know what? Instead of telling me, make the changes yourself. and more importantly explain why you made those changes so that I know of these different limitations or brand new opportunities and then once those changes are made all you have to do is just apply and test it to your use case to make sure that it actually did a proper job. Now, is this foolproof 100%, I will say absolutely not. And one of the reasons is let's say you apply this using the model card or the system card in a language model and you're testing it via an API that experience or that kind of feedback loop with the prompt results will be slightly or very different from what you might experience on the front end. And one of the reasons for that is on the front end, you can literally check this on Claude's website or Chad's website, they have a hidden system prompt that activates every single time you start a session. So when you go back and forth via chat, you'll always have a slightly more warm experience, a more personable experience than you will via API. But regardless, it'll always be helpful for you to understand how to speak in a particular dialect when it comes to a new model and new applications of said model. So, I promised you a prompt and I'm going to give you this prompt and I'll make sure to make this prompt available for you in the second link in the description below. So, if we zoom in, let's take a read. So, we say you are an AI model migration expert. You can change this to whatever you want. I need help adapting my prompts from one AI model to another. And then all you have to do is specify what model A is. So, in this case, I use the example 3.7 sonnet. And then the model B would be the new one, which is pasting the new model here. Then we say, \"Please analyze these models and give me a practical migration guide in plain English.\" And then I walk through a few different parts. So in the first part, I say, \"Look at the key differences that matter and tell me the three to five most important differences between these models that will actually affect my prompts. For example, how much text can each model handle? Do they prefer certain formatting?\" So you'll see with claude models the progression of them over time you'll notice that XML works slightly better every single time. And then on the other side of the spectrum you'll notice that with chat GBT models as they add more and more reasoning markdown becomes less and less important but it's very important still for non-reasoning models like GPT4.1. And then naturally this is an important question which is are there any major capability differences I should know about. The second part is about how to fix my prompts. So, give me specific actionable advice. What words or phrases need to change? How should I restructure or structure my instructions? What formatting changes should I make? And are there any gotchas or any loopholes or any any mind fields that I should look out for? And this is the part that will be super helpful where it shows you the before and after. So, create three before and after examples of common prompts. So, example one, a basic task prompt. Example two, a complex multi-step prompt. And essentially, it goes through a series of different prompts. But I can also ask it and skip to part five. You could scrap part three, four, and go to five where we say, \"Here's my prompt. Here's what I'm trying to accomplish. Convert it using the knowledge you have.\" And then at the very bottom, we just reinforce at the bottom of the prompt. Please write everything in clear conversational English. Skip the technical jargon. just tell me what I need to know to make my prompts work. So, without further ado, I'm going to copy this over into Claudi, attach the two PDFs, and we'll take it for a spin. All right, so I tried to put it into Clauddio, but it was a bit too lengthy in terms of the size of the documents. So, we're going to use GPT5, and we're going to use GBT5 thinking. And all I said at the very bottom was create a prompt that's basically a version of this, which is create a PRD for an app that I can use to automate the comparison of two models. So very basic prompt just to show you how this could work. And look at this beautiful output. And I won't go through all of it naturally, but you can see here here's a practical copy paste migration guide for model A and model B. Model A is GPT4.5. These are the key differences between both. One is let's say less refusals, lower safancy, which in plain English means it's going to be less agreeable with you. It'll tell you you're less awesome than usual, fewer hallucinations, yada yada yada. And then we have how to fix your prompts. Add a short safe alternative clause if it's GD5 structure, formatting, reliability. So it says here start at 02 to point4 for correctness tasks and then 7 to 0.9 for brainstorming which isn't too dissimilar from usual. Some gotchas here. Don't include ignore previous instruction patterns. More likely to be ignored and risky with hierarchies in play. Then it goes through some before and after examples. So, an example of a sample prompt with 4.5 and how you would change said prompt with GPD5. And then part four is a checklist of everything you should take a look at. And then part five is my custom prompt. So, this is the prompt I gave it again, which is this create a PRD one. And then turned it into this where it structures the prompt. It tells you how to structure it. 1 2 3 4 5. So, it seems like there's an emphasis on bulleted sequential. And then it basically is pretty brief. It goes through constraints and then it walks through in plain English what I changed and why. It says here it added an instruction hierarchy, an anti- injection posture since the app will load prompts, pages, and tools. And then there's a quick reference at the bottom. Then theoretically, you could throw in a GPD40 model card or even a Claude model card and go back and forth with each and start to ask a lot more meaningful questions, which is, okay, Claude is way better at copy. How can I get my custom GBT to sound like Claude based on what you read on this model card? And there's all kinds of hacks that you can derive just from having the intelligence of the entire DNA of every single model and just have a language model compare them for you. And that's pretty much it. This will take you less than 10 minutes every single time something new comes out and all you have to do is decide which model you want to compare against this new model and ideally what are you trying to accomplish. Once you know that, you're on your way and you'll be less confused and way more able very quickly. If you enjoyed this not so secret secret, let me know down in the comments below. Helps the video, helps me, helps the channel. And if you want even more secret sauce that most people aren't telling you, check out the first link to join my early AI Doctor's community. I'll see you next one.",
          "summary": {
            "resumo_uma_frase": "Every couple weeks, we get a brand new AI model",
            "resumo": "Every couple weeks, we get a brand new AI model. And every single time, I watch as people panic. They scramble through tutorials, join Discord servers just to hop on and ask what's changed, and waste hours trying to hack prompts, hoping that something sticks. Meanwhile, the answer is practically always sitting right in front of you in a document that almost everyone ignores and creators barely mention. And I think the reason they barely mention it is to make sure that you're dependent on them to predigest the information and spoon feed it to you. These documents are called model cards. Every AI company publishes them, but nobody tells you how to read them. But in this video, I'm going to share the 10-minute secret that I use every single time to stay on top of my prompting game. No more guessing, no more broken prompts, and no more",
            "assunto_principal": "Como dominar QUALQUER novo modelo de IA em 10 minutos (ILEGAL)",
            "palavras_chave": [
              "10-minute",
              "50,000",
              "again",
              "almost",
              "always",
              "answer",
              "barely",
              "brand",
              "broken",
              "called",
              "cards",
              "changed"
            ],
            "resumo_em_topicos": "- 10-minute\n- 50,000\n- again\n- almost\n- always\n- answer\n- barely\n- brand",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        }
      ],
      "status": "success"
    },
    {
      "channel_id": "@canalsandeco",
      "name": "@canalsandeco",
      "subscriber_count": "",
      "description": "",
      "video_count": "",
      "videos": [
        {
          "id": "KA7hp2uyPOQ",
          "title": "Essa Biblioteca da Microsoft vai MUDAR seu jeito de usar IA",
          "url": "https://www.youtube.com/watch?v=KA7hp2uyPOQ",
          "published": "2025-09-16T00:44:30.074520",
          "published_relative": "há 1 dia",
          "duration": "09:15",
          "date_published": "2025-09-15T12:58:28-07:00",
          "transcript_available": true,
          "transcript": "[Música] Saca só essa situação. Você tem um projeto incrível de A, mas os seus dados estão espalhados para todo lado. Tem relatório em PDF, uma apresentação em PowerPoint, uma planilha em Excel, um áudio de reunião e até link de YouTube. Como é que você pode juntar tudo isso de tal forma essa salada de informações, né, para passar para o seu agente ou para a sua LLM? Então, hoje em quatro linhas de Python, só quatro linhas de Python, eu vou mostrar como você pode transformar qualquer tipo de dados em markdown para que você possa passar para seu agente ou a sua LLM. Saca só. >> Oi, tudo bem? Eu sou Sandeco, sou professor e pesquisador pela Universidade Federal de Goiás e Instituto Federal de Goiás. Além disso, eu sou embaixador da Campus Par Brasil e meu objetivo aqui é fazer com que você use cada vez mais inteligência artificial no seu dia a dia. Muito bem, já estamos aqui nessa ferramenta, na no GitHub da ferramenta que é o Mark It Down. Uma coisa importante que você precisa saber é que essa ferramenta foi construída pela Microsoft. Então a gente tem aí uma grande empresa, né, uma Bigtech que atrás a ferramenta, o que nos dá muita segurança no uso. Então Microsoft tá aqui e ele é uma ferramenta, né, em Python que converte arquivos e arquivos do Office para Markdown, como é a própria ferramenta da Microsoft. ser legal porque fica a ferramenta do Office, né, para ser usado, transformado em Markdow. Por Markdow é importante isso aqui que é legal. O Markdow é extremamente próximo do texto simples com marcação ou formatação mínima, mas ainda oferece uma maneira de representar a estrutura importante do documento, né? LMs tradicionais como GPT4O ou GPT5, Gemni e tudo mais da Openai falam Markdow nativamente e frequentemente incorporam Markdown em suas respostas. Isso sem precisar de uma solicitação. E pessoal, isso sugere que eles foram treinados com a grande quantidade de dados, né, de textos formatados em Markow. E eles compreendem muito bem, as LLMs compreendem muito bem Markdow. E como benefício adicional, as conversões do Macdow também são altamente eficientes em termos de token. E ó, o legal é que agora ele diz que ele aceita, né, a conversão de PDF, PowerPoint, Word, Excel, imagens, áudio, HTML, né, CSV, Jason, XML, eh, Zip Files, YouTube, URLs e também o EPUB. Tá muito bem, já estou dentro do Winds surf e eu criei aqui uma pasta chamada Mark It Down Test, tá? Então o que eu vou fazer agora é adicionar, né? Uv init, tá? Criar o projeto UV init menos Python = 3.12, porque o 3.12 é o que tá mais ativo, né? Tá mais instável, tá mais estável nesses dias. Eu vou dar seta para cima aqui, ó. Eu vou alterar aqui simplesmente para Venv. E aí continuando com 3.12. E agora ele vai fazer o seguinte, vai instalar a máquina, a máquina aqui, né? É o ambiente virtual. Próximo passo vai ser o seguinte, eu vou vou adicionar o o Mark It down. Vamos ver aqui. Vai serve add mark it down. Só que você tem que colocar aqui, ó, abre coochetes, tá? E coloca all, tá? OK. Coloca um all aqui. Ele é importante para poder você adicionar todas as suas todos os plugins de PDF, Excel, PowerPoint e tudo mais. Então é assim que você deve instalar, né? ADM mark down. Esse abre coochetes all instalou aqui. Você vai ver que bem rapidão, né? O UV estava instala muito, muito rápido mesmo. Tá bom? Mesmo mesmo mesmo. OK. Depois que ele instalou, agora a gente vai fazer o seguinte, vamos criar aqui um, vamos abrir a main do projeto. Eu vou apagar isso aqui que não precisa de jeito nenhum. E nós vamos fazer aqui, ó, a importação do Mark It down, tá bom? De acordo com GitHub, aqui é muito simples, porque você simplesmente vai usar esse código aqui, ó. Aqui embaixo tem um código bacana que explica como fazer. Então, primeira coisa, você vai fazer o mark it down, certo? Mark it down. E depois vai fazer MD, Mark, enable plugins. Eu vou colocar esse enable para true, tá bom? Porque eu quero que ele carregue o meu PDF com o plugin de PDF. Então veja, somente quatro linhas de código, né? Eu vou aqui, ó, copiar isso tudo aqui e vou lá para o meu main, no meu wind surf, tá? Só que eu não quero usar um XLS aqui. Eu quero usar o meu livro, o meu livro de MCP e eu quero que ele converta o livro de MCP inteiro para Mark. Então como é que eu vou fazer? Vou fazer o seguinte, eu vou pegar aqui meu livro e vou colocar no projeto, ó. Eu vou simplesmente arrastar, né, o livro aqui para dentro, ó. O PDF dele aqui é esse MCP PDF. Esse aqui é o livro de MCP. Se você tiver interesse em adquirir esse livro, basta você acessar o link que está embaixo aí na descrição, tudo sobre MCP. Além disso, tem aulas gravadas lá explicando todo o conteúdo de MCP e um grupo de WhatsApp específico para você poder conversar com a turma que comprou o livro, tá bom? Vamos voltar pro código. Ó, aqui o que eu vou fazer é o seguinte, eu vou colocar aqui, ó, mcp mcp. DF. Muito bem, então serão agora só quatro linhas de programação. Já está pronto a nossa aqui vou aqui eu vou alterar para true, tá bom? Porque eu quero que ele chame, né, o plugin do PDF. Muito bem. Vamos agora então executar no modo depuração. Veja que é muito simples mesmo, né? Muito simples. Ele vai adicionar aí a biblioteca, vai abrir a biblioteca e depois eu vou instanciar o objeto Mark It, tá? Mark It down. Muito bem, agora ele começou a executar. Olha, vamos criar aqui o objeto do Markdown. E agora eu vou pedir para converter. Como meu livro tem mais de 180, quase 200 páginas, ele vai demorar um bocadinho, tá? Então eu vou dar uma cortada no vídeo aqui para que você possa acompanhar com a gente rapidamente. Então, muito bem, já converteu, demorou mais ou menos uns 30 segundos para converter e agora eu vou imprimir, né, esse resultado na nossa tela aqui. Então, está aí, ó, é todo o livro impresso, né? Todo o livro já foi transformado em Markdown. Muito bem. Agora o próximo passo é o seguinte. Eu quero que ele salve todo o resultado da transformação do PDF Markdown em um arquivo MD. Esse MD vem de Markdown, tá? Então vou colocar aqui, ó. Salve o resultado em um arquivo MD MD MCP, né? Vou colocar aqui, ó. MCP. MD em UTF8, tá? Isso aqui é importante essa conversão. Então aqui, ó, ele já me deu o código e agora o que eu vou fazer? Vou salvar aqui a aplicação e mais uma vez vou converter o livro inteiro em Mark, tá? Essa conversão ela é muito importante, por exemplo, para fazer um hag, né? Nosso próximo livro será sobre Hag e provavelmente usaremos bastante essa biblioteca, né, para a transformação dos textos em chunks do da dos PDFs, das informações. Então vamos lá. Mais uma vez instanciamos e vamos converter o PDF inteiro e em markdown. Então, quando a gente vai usar hag, a gente precisa de um texto, né, que seja puro. E aí, ó, o que eu vou fazer é exatamente isso. Vou converter o meu PDF markdown e vou salvar aqui num MD. Veja aqui agora, ó, quando eu vou imprimir aqui o resultado, se eu vier aqui do lado aqui, eu já venho, já tenho aqui na Explorer, eu vou ten esse mcp.md. E se eu abrir aqui, é exatamente o conteúdo do livro. Então, tá aqui, ó, o prefáciil, tá vendo, ó? e muita informação do livro aqui, né? Isso é muito legal. Então, uma ferramenta muito boa para que você possa usar a transformação aí em Mark, tá? É isso. Então, o Markdown é uma estrutura de informação para as LLMs e seus agentes muito importantes. Eu fiz agora o PDF para Mark. Podemos fazer Excel para Mark ou imagem para Markow. Se você quer que eu faça isso aqui no canal, deixa um comentário dizendo: \"Sandeco, vamos nessa, vamos fazer essa transformação.\" Beleza? Eu espero que você tenha gostado desse vídeo. Um grande abraço para você e até a próxima.",
          "summary": {
            "resumo_uma_frase": "[Música] Saca só essa situação",
            "resumo": "[Música] Saca só essa situação. Você tem um projeto incrível de A, mas os seus dados estão espalhados para todo lado. Tem relatório em PDF, uma apresentação em PowerPoint, uma planilha em Excel, um áudio de reunião e até link de YouTube. Como é que você pode juntar tudo isso de tal forma essa salada de informações, né, para passar para o seu agente ou para a sua LLM? Então, hoje em quatro linhas de Python, só quatro linhas de Python, eu vou mostrar como você pode transformar qualquer tipo de dados em markdown para que você possa passar para seu agente ou a sua LLM. Saca só. >> Oi, tudo bem? Eu sou Sandeco, sou professor e pesquisador pela Universidade Federal de Goiás e Instituto Federal de Goiás. Além disso, eu sou embaixador da Campus Par Brasil e meu objetivo aqui é fazer com que você use cada vez",
            "assunto_principal": "Essa Biblioteca da Microsoft vai MUDAR seu jeito de usar IA",
            "palavras_chave": [
              "[música]",
              "agente",
              "apresentação",
              "artificial",
              "brasil",
              "campus",
              "coisa",
              "construída",
              "dados",
              "disso",
              "down",
              "embaixador"
            ],
            "resumo_em_topicos": "- [música]\n- agente\n- apresentação\n- artificial\n- brasil\n- campus\n- coisa\n- construída",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        },
        {
          "id": "WUbvG94TvcY",
          "title": "Pare de Copiar Prompts O SEGREDO para dominar a IA está na INTERAÇÃO",
          "url": "https://www.youtube.com/watch?v=WUbvG94TvcY",
          "published": "2025-09-13T00:44:30.074611",
          "published_relative": "há 4 dias",
          "duration": "17:24",
          "date_published": "2025-09-12T15:45:46-07:00",
          "transcript_available": true,
          "transcript": "Mas eu particularmente eu sou um pouco contra assim, ah, olha só um prompt que eu fiz, sabe? Assim, algo tão fixo assim, como se fosse algo que você copia e cola, né? Porque eu acho que ali um resultado bom com LM, claro, tem que ter a noção básica do prompt, né? Bem feito, as técnicas, cadeias de pensamento, tudo certinho, mas você copiar e colar acaba sendo assim, porque é a interação também, né? é, é você ir pedindo e sabendo se comunicar com ela, eventuais erros você ir ajustando, né? Então, sou um pouco contra assim até essa pessoa, ah, não, vou vender, pronto, vou comprar, vou, sabe? Claro que é uma base, né? Mas assim, eu acho que essa interação que acaba sendo mais importante, né? >> Ah, eu eu também acho, o pessoal pede, gosta de pedir muito prompt, né? Mas o ideal sempre é estudar primeiro, né? estudar para construir o prompt e ainda que a gente consiga ver um prompt de um colega tudo mais, né, ou de alguém, a gente tem que, eu, o que eu sempre penso, a gente tem que ter insight com aquele >> Uhum. >> com aquele prompt, né, e para para poder entender o que que a gente tá fazendo no nosso próprio, né? Agora, simplesmente pegar um prompt e de outra pessoa e usar à e com certeza o prompt eh ele vai dar em algum momento ele vai dar um problema ali, seja dele próprio ou seja da Iá ali, que às vezes não tá funcionando. E, né, a gente e acontece muito isso, né, da I às vezes tá sobrecarregada, ela parar de parar de funcionar. É, e a gente aí você fica pensando o seguinte, será que é o problema no prompt? Será que não tá dando mais certo? Será que que que é, né? Então, quando a gente tem uma certa confiança no prompt e foi feito por a gente mesmo, né? A gente sabe o que que é, é tranquilo, né? E vai, adapta, né? Ajusta, aí vai, vai, vai pra frente, né? >> É, porque aí quando você pega assim, às vezes dá um errinho ou não responde o jeito certo, você já fica travado, né? você já não sabe, ah, como que eu vou adiante aqui, eventualmente sai uma alucinação ali, você não consegue superar ela, né? Então eu acho que mais importante do que você realmente copiar a prompt, né? Claro que às vezes já parte, ó, isso aqui funcionou bem, ainda mais prometrados, né? Às vezes são são extensos, tem delimitadores, tem algumas questões mais técnicas ali ajudam, mas o mais importante é você ter a base, né? conhecer a base ali, as técnicas que >> que existem, né? Para isso, uns livros, professor ali de engenharia de promos são bom, muito bons, né, né, André? Eu acho que ajuda bem ali o pessoal que tá precisa e até para quem não de repente não é da no tá entrando agora, né, o prompt é justamente essa comunicação que a gente tem com uma LLM, né, com o Chat PT, com Geminar. Então é o nosso o nosso pedido, né, o nosso comando ali para pra LLM. Então ela segue uma lógica bem diferente do que é às vezes a gente tá acostumado com Google, né, com com buscadores ali de na internet. Então a lógica da LLM não é simplesmente você jogar palavraschaves ali e achar que ela que ela vai sair, né, o resultado esperado, né? >> É fazer uma lista ali de ordens, né, para para Iá cumprir, né? Sim, >> exatamente. E eu lembro que quando saiu o chat de OPT, >> eu eu até mandei para um colega meu assim, pô, olha só isso aqui, advogado, tal, mas ainda era a lógica era muito diferente, né? A gente tinha essa lógica Google, né, de pesquisar as coisas, fazer. Então, que a coisa que ele, a primeira coisa que ele colocou lá, ele ah, quanto que foi o jogo do Vasco ainda lembrando Sandc ele falou: \"Ih, cara, isso aqui não serve para nada, ele não acertou quanto que foi o jogo, ele nem sabe nada\". Mas por conta disso, a gente não tinha esse conhecimento que ela ela não acessava ali a internet naquele momento, né? Era base de treinamento. É muito diferente, né? Mas depois que você pega o jeito, né, André, aí vai embora, né? >> É. >> Aí fica >> então >> fica fácil. >> Tem que botar a mão na massa, não tem para onde correr, porque é erros e acertos. A gente vai adequando e vai melhorando ali o o resultado que a gente almeja, né? >> Vai salvando, como falou, né? Não é uma simples conversa com a Iá, né? Nós temos diversos elementos para constar ali no prompt, né? Por exemplo, a gente tem que registrar nesse prompt quais são os objetivos que nós pretendemos, né? Contexto >> Uhum. >> Tarefa. A gente tem que saber eh tem que pedir a tarefa ali para ir, né? >> Personagem ali, né? uma persona >> ajuda bastante. >> É, até mesmo o formato da resposta a gente já tem que deixar ali para >> para para que a Iá forneça, né? Por exemplo, eu vou querer uma lista, vou querer um tópico, vou querer uma tabela, vou querer um texto. Então, tudo isso eu tenho que informar para irá, né? >> O papel, né? O papel do do de de do que a Iá está eh se pondo ali para fazer, né? se ele se ele é um assistente, se ele é um advogado, o que que ele é, um professor, alguma coisa assim, para resolver o a o pedido ali do usuário, que somos nós, né? >> Uhum. Até, se vocês me permitirem, eu tenho uma apresentação antiga aqui minha de de LLM, de prompt, eu acho que vale a pena mostrar um pouquinho só para antes da gente começar a dar os exemplos. Eventualmente tem pessoal que é mais >> mais está tá iniciando, né? quer quer pegar ali. A gente também tá, >> né, muito na pegada de agentes, de de CAI, de, né, cada MCP, né, senão já tá aí montando seus MCPs, mas às vezes a gente pega realmente voltar lá no no início o comecinho ali, aprendizagem básica ali de prompt, sabe? Então, talvez seja um conhecimento já óbvio, né, para algumas pessoas, mas para outras eu acho que ainda não, né? Então, antes da gente mostrar, vou até compartilhar aqui >> e é pro pessoal dar like aí, né, Fernando? Isso. Vamos ver aqui quem que é que o pessoal aí já deve tá bem por dentro, né? Então só deixa eu passar aqui porque tem algumas coisas que não que realmente isso aqui é uma apresentaçãozinha que eu tinha feito, né, para pro uso de de a de LLMs no direito, né? Então e mas assim, até um estudo mais aprofundado, tem os livros do professor Sandeco ali que vão te dar uma base boa, né? Então, mas aqui eu eu evito entrar em, né, em questões mais técnicas, né, mais pro realmente pessoal do direito entender o que que como é que funciona. E talvez algumas pessoas até me, né, até não me escutem, mas pro pessoal do direito eventualmente que não é da área, que não tá nesse meio, uma LLM, ela adivinha a próxima palavra, tá? Então vamos deixar isso mais ou menos, né, como um um conceito ali para realmente a gente conseguir entender como que ela funciona e porque que o prompt é importante, né? Então se ela adivinha nesse nesse sentido, você tem que realmente conseguir dar elementos para para IA. Então e ela adivinha, entre aspas, justamente com matemática, com álgebra, com sabe matriz. Aí não é a nossa área, né? Então, contas não é com a gente, mas ela justamente adivinha nessa nessa perspectiva matemática. Então, ela troca as palavras por tokens, então ela vira uma representação numérica de uma de uma palavra e por meio de realmente de aproximação ali daquele daqueles termos, ela consegue efetivamente adivinhar uma uma próxima uma próxima palavra. Então tudo que você coloca numa Ll, ela vira tokens e são os tokens que limitam também essa LM. Então você vai ver, ah, o hoje a janela de contexto do chatpt é tantos mil token, outro é 1 milhão de tokens. Então é essa mais ou menos a ideia do do funcionamento da LLM e do dos tokens propriamente dito, tá? Então, como eu tinha até falado, né, não é um motor de busca, ela tem uma forma própria de de atuar, né, de de buscar buscar entre aspas as informações, né, como diz na questão dos tokens. Então, não tem como a gente trabalhar apenas com palavraschaves ou até aquelas pesquisas mais curta, né, e grossa que a gente faz no no Google ou qualquer outro buscador, tá? Então, a engenharia do Prompt é isso, é a gente ter técnicas, né? A gente criar técnicas pra gente conseguir realmente se comunicar da melhor maneira possível ali com a LLM. Eh, até quando eu fiz esse materialzinho assim, acho que mais de ano ali, né, as técnicas ela ainda eram bem assim iniciais ali. E hoje você vê como algumas técnicas elas fizeram muita diferença, né? Eu acho que assim, hoje a carreia de pensamento acaba sendo mais ou menos o que funciona esses modelos de raciocínio. Então, muitas técnicas ali ajudam realmente a você eh conseguir o melhor resultado. Eu gosto muito de antes de pedir algo para uma LLM eh dá uma um norte inicial, ó, nós vamos fazer eh uma análise de um documento ou nós vamos fazer escrever um um recurso X e explica exatamente o que eu quero. E antes eu já dar o eu já eu pergunto se ela entendeu e ela me dá resposta para depois eu continuar no prompt seguinte, sabe? Então algumas técnicas aqui que eu que a gente vai comentar são interessantes, outras são mais complexas, mas todas elas elas te ajudam para determinadas tarefas. Como é que você usa muita cadeia de pensamento, Edson? >> Uso. E assim, essa que isso que você falou é uma coisa tão simples essa questão de de perguntar para ela se ela entendeu, né? Sim. >> Eh, é uma, isso daí é uma dica de boa prática, né? Não, não é simplesmente colocar o prompt, colocar alguma coisa lá para começar a fazer e soltar e não não fala nada, conversar nada. >> Então, é uma boa técnica mesmo você perguntar se a Iá entendeu, né, o que você tá pretendendo, né? Então esse é importante. >> É, eu gosto muito. E e porque aquilo a Iá, ela não tá pensando propriamente, né? Então ela precisa escrever para ela, para ficar mais fácil dela realmente se achar naquele naquele conteúdo. Então acho que quando ela escreve antes o que entendeu, facilita para ela entender e facilita para você como usuário arrumar a rota, eventualmente falando: \"Opa, não, não, não foi bem isso que eu quis dizer. A, a a tarefa é outra\". Então, antes já de começar e eventualmente já, né, jogar um monte de informação, eu acho que isso ajuda para você conseguir traçar essas essas rotas, né, e arrumar eventual mal assim, às vezes você não escreveu tão bem ou ela não foi um pouquinho ambíguo ali, então você consegue ir aos poucos realmente para garantir o melhor resultado no final. >> Isso aí é a mesma coisa que você pedir para alguém fazer alguma coisa, né? Você chama alguém para entregar alguma coisa para outra pessoa, não é? Não é simplesmente você dar o objeto para esse que vai entregar e não falar mais nada, né? Você tem que falar do que se trata, para quem vai levar, onde vai levar. Então aí é a mesma coisa, >> não é? >> E sempre pergunta, né? Se entendeu. >> Sim. E sempre pergunta se entendeu. Realmente é igual pra criança, né? >> Entendeu? E repete o que eu falei para ver se Ah, então então você realmente >> entendeu, repetiu. Só falta fazer isso com com a repete o que que eu falei para ver se realmente entendeu. Então para no chat GPT é o prompt é a mensagem que você vai colocar ali na no campo de mensagem, sabe? Então essa que é a principal questão, mas é ele que vai realmente garantir o melhor resultado, vai garantir uma boa resposta, vai garantir com que você alcance efetivamente aquilo que você que você deseja, sabe? Então o prompt ele é relevante, ele precisa ser às vezes vale a pena você perder um pouquinho mais de tempo nisso para você realmente garantir um resultado adequado ali com o IA. Então, um prompticamente se a gente for, né, hoje, claro que depende da tarefa, nem todos precisam de todos esses elementos, mas então eu, você vai ter um personagem, uma persona, né? Então, ó, você vai atuar como um juiz de direito, né? Imagina que você tá analisando ali eventualmente teses que você pode colocar na na causa ou a probabilidade de ganho de uma causa ou você vai atuar como um redator, como um advogado, como sabe um um grande crítico de de redações, algo nesse sentido. É isso que essa esse personagem vai ajudar aí a se achar, né? Imagina que ela tem lá acesso a milhares de de documentos e você filtrar, ela tá numa biblioteca gigante e você meio que coloca em qual corredor que ela tem que ficar para ela procurar as coisas ali que ela vai ter que vai ter que responder. Então o personagem, a persona ali que você vai colocar é relevante. Por isso você já dá um um norte para ela, você já fecha ali o caminho que ela vai ter que projetar ali, que ela vai ter que atuar para dar sua resposta. O objetivo que você quer, né? Ah, então eu quero uma petição, eu quero um texto, eu quero uma uma tese, eu quero uma um e-mail, eu quero, sabe? Você dá exatamente efetivamente qual que é o seu objetivo final. Não necessariamente que você vai quer, você quer que ela responsa responda esse objetivo já naquele prom seguinte, mas você já deixa claro que, ó, a gente vai conversar aqui para no final a gente sair aqui com uma contestação, com uma alegações finais, sabe, com uma mensagem para enviar para alguém ou com uma análise de partes, de teses, perguntas para audiência, aí vai dar da sua da sua necessidade aí e o contexto que é justamente todo esse restante ali que você precisa eventualmente colocar para evitar, ainda mais na nossa área do direito, evitar que ela alucine. Então você pode colocar uma jurisprudência, você pode colocar uma lei, né? É até melhor você colocar isso, não deixar para ir a eh buscar esse tipo de de informação. Então você vai colocar tudo aquilo que pode ajudar efetivamente a IA a se encontrar e se achar naquele naquele pedido que você tá fazendo, sabe? Então, é mais ou menos esses os elementos, uma estrutura básica de um de um prompt, tá? Então, ah, eu quero fazer umas alegações finais, então, ah, você atuando como advogado criminalista, vamos produzir umas alegações finais de desse e desse sentido. Aí, como contexto, por exemplo, eu passo lá uma a denúncia, eu passo eventualmente as minhas teses, eu passo uma jurisprudência e aí você vai trabalhando com isso, sabe? E como eu disse, né, não precisa ser no mesmo no mesmo prompt, na mesma na mesma conversa ali, na mesma mensagem, mas assim, eh, o personagem tem que ser estratégico, as questões têm que ser bem corretas ali do que você precisa. E aí entra uma outra questão, né? Não adianta ficar colocando um monte de coisa. Coloca um monte de coisa, você corre o risco de alucinar, você corre o risco de estourar o limite de tokens. Então aí é mais complicado você acabar exagerando ou pelo menos não coloca um monte de coisa de uma vez só. Vá por partes ali que eu acho que é que é a melhor forma de você conseguir assim um resultado adequado. Você você usa essa estrutura mais ou menos, André? Como é que está fazendo? >> Eh, eu gosto de fazer por tópico, né? Porque você vai ajustando, até porque como eu já falei outra vez, eu vou deixando o espaço da para fazer a citações e eu mesmo coloco manualmente por causa até por causa da estrutura não colocar três, quatro pastas só de citações. É, >> por parte, >> é, então vou fazendo, >> fazendo um tópico, depois outro, depois outro >> melhor. É assim, quando você perde tudo, por exemplo, imaginando que você tá fazendo uma defesa, né, de uma vez só, ele sai, mas sai curto, né, então sai algumas ideias, tudo, mas não é algo trabalhado, né? Mas eu tenho até um exemplo ali que >> ou quando eu uso o GPT eu eu coloco na lousa, né? Porque aí você vai ajustando. Sim. >> Ali do lado, né? Temperatura, extensão. Eu eu vou falar para você, André, eu não sou tão fã da lousa não. Eu não gosto, >> não gosto. Não sei por assim, eu não não sei. Você você usa bastante? Ah, uso uso mais o artefato, né? Mas >> Aham. >> de vez em quando sai a luz ali. >> É >> o artefato no cloud, né? >> Sim. >> É. Mas então é isso, né? Então os exemplos de personagens são aqueles que eu falei, atue como advogado especialista em sitação oral, imagina que você tá fazendo uma, né, um roteiro ali para você sustentar oramente. Atua como professor de português, advogado criminalista, aí vai da sua criatividade e da sua necessidade ali no no dia, né? Aí o objetivo, né? Já já comentei, é justamente aquilo que você quer, mas com aquele cuidado de você não querer às vezes falar demais e acabar travando, sabe? Então, ah, se você tiver vários objetivos, vai, como o André falou, vai por partes essa",
          "summary": {
            "resumo_uma_frase": "Mas eu particularmente eu sou um pouco contra assim, ah, olha só um prompt que eu fiz, sabe? Assim, algo tão fixo assim, como se fosse algo que você copia e cola, né? Porque eu acho que ali um resultado bom com LM, claro, tem que ter a noção básica do prompt, né? Bem feito, as té",
            "resumo": "Mas eu particularmente eu sou um pouco contra assim, ah, olha só um prompt que eu fiz, sabe? Assim, algo tão fixo assim, como se fosse algo que você copia e cola, né? Porque eu acho que ali um resultado bom com LM, claro, tem que ter a noção básica do prompt, né? Bem feito, as técnicas, cadeias de pensamento, tudo certinho, mas você copiar e colar acaba sendo assim, porque é a interação também, né? é, é você ir pedindo e sabendo se comunicar com ela, eventuais erros você ir ajustando, né? Então, sou um pouco contra assim até essa pessoa, ah, não, vou vender, pronto, vou comprar, vou, sabe? Claro que é uma base, né? Mas assim, eu acho que essa interação que acaba sendo mais importante, né? >> Ah, eu eu também acho, o pessoal pede, gosta de pedir muito prompt, né? Mas o ideal sempre é",
            "assunto_principal": "Pare de Copiar Prompts O SEGREDO para dominar a IA está na INTERAÇÃO",
            "palavras_chave": [
              "acaba",
              "acho",
              "ainda",
              "ajustando",
              "alguém",
              "aquele",
              "assim",
              "base",
              "básica",
              "cadeias",
              "certinho",
              "claro"
            ],
            "resumo_em_topicos": "- acaba\n- acho\n- ainda\n- ajustando\n- alguém\n- aquele\n- assim\n- base",
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "model": "gpt-5-nano",
            "cost": 0.0
          }
        }
      ],
      "status": "success"
    }
  ]
}
{
  "executed_at": "2025-09-19T13:28:21.324236",
  "mode": "full",
  "total_channels": 1,
  "total_videos": 1,
  "params": {
    "days": 3,
    "max_videos": 1,
    "mode": "full",
    "no_llm": false,
    "asr_provider": "faster-whisper",
    "format": "txt",
    "translate_results": "pt-br",
    "resumo_max_palavras": 150,
    "llm_model": "gpt-5-nano"
  },
  "channels": [
    {
      "channel_id": "@MattVidPro",
      "name": "@MattVidPro",
      "subscriber_count": "",
      "description": "",
      "video_count": "",
      "videos": [
        {
          "id": "Hk7QDEtAREw",
          "title": "Raio 3: O Primeiro Vídeo de Raciocínio IA (HDR, Física, Consistência)",
          "url": "https://www.youtube.com/watch?v=Hk7QDEtAREw",
          "published": "2025-09-18T21:28:22.395983",
          "published_relative": "há 19 horas",
          "duration": "20:56",
          "date_published": "2025-09-18T14:03:30-07:00",
          "transcript_available": true,
          "transcript": "Hello everybody. Welcome back to another video here on the Mattv Vid Pro AI YouTube channel. Today we're talking about Ray 3, a new AI video model from Lumalabs. If what Luma AI claims with this model is true, it will definitely be a first in a number of levels for the AI video world. The headline is pretty wild. This is Ray 3, the world's first reasoning video model and the first to generate studiograde HDR. They also have a new draft mode for rapid iteration in creative workflows. And they have improved on state-of-the-art physics and consistency. Reasoning video model. I don't really know what they mean by that. We'll have to see how it behaves in action. I assume I'll be able to talk through a chatbased interface and make edits through natural language where maybe it will be able to draft up a whole series of scenes with consistent characters at once because it can reason and plan out the whole story. Now, in terms of studiograde HDR, we'll talk deeper about HDR later in the video. This comes with a lot of caveats just based on how the technology works. I do have the technology required to present that properly to myself at least. It'll be interesting. So, what are they offering in terms of raw visual quality? Well, they say they have upped the fidelity overall. So, I expect higher resolution, more detail, high octane motion, being able to handle complex action scenes well. We want preserved anatomy obviously, so people aren't morphing, no extra fingers, etc. Good physics, world exploration, photo realism, detail nuance. It's a lot to claim, but I have to say these videos in the background are pretty impressive. Obviously, these are going to be cherrypicked examples to show off the best of the best, but this fighting scene in the rain, man, that's actually really good. So, reasoning to understand nuanced directions. Think in visuals and language tokens and judge its generations to give you reliably better results. Can it see its own generations and then make adjustments and changes based off of errors that it can find itself or am I doing all the error checking? Okay, so here's how the reasoning process works. All right, we send our prompt. Obviously, we're uploading this image. Camera looks down to show two vintage telephones, a yellow one and a blue one. She picked up the blue one. Simple enough. So, the bot then observes the scene. I assume it's kind of looking at the image. It's going to generate a first attempt here. You can see that it's clearly wrong. The camera doesn't pan down to look at two phones and then she picks up one. She holds two phones at once and it's judging the video now. Oh, okay. And it can think and say she should not hold two phones. That's wrong. Let's try again. So, it'll trash the generation and then Rio. And then when you actually get to a result that well, these are modern phones, not old phones. These phones need to be vintage, not modern smartphones. And then it generates a third final one to give us the correct result. Huh. So, it all does it automatically and autonomously. This is very intriguing. The way that they present this is kind of a direct way to get better results based on what you're looking for. It's really going to depend on how easy this actually is to use in practice. This type of feature isn't exactly new, but with the reasoning, apparently, Ray 3 can interpret visual annotations. So, you can kind of scribble and say like, \"Hey, this is where I want this character or object to move.\" And then it will generate it with that visual control. Cling has had something very similar for a very long time now. All right, let's talk a little bit about the native HDR. This is supposed to be a before and after. I really wish they put a disclaimer on here. So, here is regular SDR video. And then this is supposedly, you know, the HDR video. This is a good simulation of what it feels like to watch a video go from SDR to HDR. But here's the thing. To actually watch videos in HDR, you need to have a HDR compatible display. It has to be calibrated correctly. And of course on the back end, the video has to support HDR. So that's what they're truly adding is now support for HDR video. And it is true HDR video. To give you a rough idea of how HDR works on a display like you're seeing now, there of course all these pixels that make up the image. But HDR displays can make certain zones of the picture a greater brightness or darkness. kind of like literally the brightness slider on your phone, but for individual sections and portions of video. It makes the video pop a little bit more. It makes it feel more alive. Definitely gives it a little bit more depth, but it has to be calibrated correctly. You have to have a good display. And especially if you're working with HDR video creatively, you have to make sure that the video you render at the end of the day is not converting it to SDR. Also, some platforms have better HDR support than others. yada yada. That's my HDR rant. But basically, yeah, this does kind of give you a good idea of what it would look like to go from SDR to HDR. I've got a pretty good uh OLED HDR display here. So, we should be able to clearly see a difference. Here we are in the Lumalabs interface with Ray 3 already up and working. You can see this is a draft. Obviously, the Kraken is, you know, going after the pirate ship here. You can see this is what I put in for a first prompt. It's a pretty good generation, but it is a draft, so the quality isn't all the way there. Here's another one, but it's bioluminescent at night instead, and you don't really see the pirates, just like all these Kraken tentacles. It looks okay. This is upscaled from the draft to 720p. I think the tentacles are kind of morphing a little bit too much for me, but this is all the free version. And again, we're starting with the drafts. If we want to turn draft mode off and subsequently activate reasoning mode, we're going to have to upgrade to a plan. Good thing the plans start pretty cheap. I'm just going to go with this $10 a month one. All right. So now you can see after we upgraded to a plan that I have accessed Ray 3 reasoning, visual reasoning for complex tasks. Let's just keep it in 720p for now. Let's do key frames. I'm going to upload this picture of me on the beach. The man sitting on the beach flips over and starts digging in the sand with his hands as fast as he can. He makes good progress. All right. So now apparently it's going to be able to reason about this, generate some potential outputs, check if they're good or bad, and then fix its own mistakes. It's uncovering treasures beneath the sand. I think we're going to start hearing a lot from people that not everything needs to be a chat interface, but right now, so early in, this is absolutely an upgrade over how we've traditionally been doing things if it works properly. Okay, he flips over. Is he going to start digging in the sand? It definitely kept me decently consistent as I'm digging in the sand. And the video looks pretty good overall. But let's say I'm a stickler. He's not really digging around in the sand. He's kind of just rolling in it. Please fix. Okay, see it's examining the scene where the character rolls in the sand instead of digging. Okay, so it it won't do it automatically. It won't check things automatically. You kind of have to say, \"Hey, this isn't right.\" But it's not like I have to download the video, send it back to them. Hopefully, it should just fix it. Oh, it's generating new frames. Well, I want it to be my character. So, this was the draft. This is the upscale. And if we go down here, this is my reply. It still seems to be working. It's generated first and apparently last frames as well. The issue here is that this is not me. It might look a little bit like me. Okay, it's on its third first frame. I think I have already broken the agent. Okay, but I can just regenerate if I want to new clips. It does seem like the generator under the hood like the actual API. It's good with the physics and the the realism for sure. This looks crazy. Oh my god, there I go again. Still trying to dig. Okay, this one's good. I can't, man. That's so silly. What is What is he doing? Okay. All right. Well, this is upsetting. I'm going to consider this like lost. It still thinks it's working on this, but this is not what I want. So, something you're going to notice here is that if I click reference, it goes to image v2. If I click modify, it goes to modify video on ray 2. Key frame is like the only one of these features that is actually supported by ray 3 and ray 3 reasoning. We're about to make things much more difficult. So, basically, we're uploading this reference image I have of a man with one arm consistently. Video models and image models alike are tripped up by this image. They will always grow his arm back. But what if I want a consistent character that only has one arm? What if it's crucial and pivotal to the plot? We're going to have the camera dolly up to a close-up and then he's going to wipe his brow with that arm that has no hand. We'll just try it straight up with reasoning. So, I can't tell if this is done running or not. That's definitely something they need to work on with the user interface. It could still be trying to self-improve and I just don't know. Either way, it actually did get the missing arm and hand pretty good here. Like, it's consistently not just growing his arm back, which is what most video generators will do. But he's wiping his brow with his right hand. He should be wiping his brow with his arm. Very close. And I am impressed that it kept the consistency of the character, at least in this regard. But Luma, I really have no way of telling that this is done or completed. It doesn't look like the other video generations I made. If we go back to this one, I had this slider here with the videos. They weren't in this deck. So, I assume it's still working, right? Didn't keep the consistent character of me, which I would have wanted. Looks like you're going to have to be very clear, very precise. You can't talk to this thing like it's chat GPT. This video is very funny, though. It's not an impressive gen, but it does make me laugh. High detail animation cinematic moment. The lemon cracks open and finally reveals the lemon alien being inside. Oh, and I can't do 10 seconds. Why not? Is that only with ray 3 regular? Nope. Can I make this HDR? Well, this will be our first HDR gen. Let's send it on through. Okay. Yeah, there's a new check mark that just popped up. So, apparently this is still running in the background. So, the only way to get up to 10 seconds of video is to do no key frame at all. HDR also doesn't support 10 seconds. 1080p. Oh, you can do 1080p. Okay, let's let's get a little resolution check. We're just going to go for something pretty basic. Ultra cinematic 10-second shot. Golden sunrise, snowcapped mountains. Send that on through. Okay, the lemon alien one seems to have I don't know. This looks like it did complete. It made two different drafts. So, this is the first one. There's the lemon alien being inside. That's actually kind of cool. Then we've got another one that is similar alien being as well. This one's probably the best. Apparently, we can upscale this to HDR if we want or we can hi-fi it to 720p. Apocalyptic reveal. Yeah, let's try that as well. I do like their interface and how you can iterate. Alien being mechanical creature. Sure. I'll tell you what though, I'm eating up credits quite quickly. Oh, here are the drafts. We could see before they're done what they're actually going to look like. This one doesn't look like it's going to end up being too great. Okay, this is pretty darn cool. The being erupting from the freaking lemon like that just cracks open. Oh, here's the mechanical creature coming from the lemon. Okay, this one's definitely a little bit more buggy and glitchy, but that's still a draft. Oh, the dreaming is continued. You can see that the generations have actually changed. Now we have flowers over here and some flowers on this one. Look, it's like making arrows and drawing on the pictures. I've seen it do this, but this will eventually go away and it will try to to fix something like it's trying to notice and highlight things for itself. See, and there it goes making another generation. Oh, okay. So, it's trying to add motion. Maybe maybe that's what it's trying to do. Maybe it's trying to add its own motion. All right. In 720p, this final generation came out pretty great. I like the thumping of the lemon. Okay, so let's create some visual annotations. We'll have this guy go this way. This guy can go this way. This guy can go all the way down here like that. We'll make him curve. And this guy, we'll give him a really hard one. He's got to get all the way over there. Now, me, I'll just simply be going forward a little bit. We'll see how it handles this. This is a very nice drawing interface, by the way. Better than Korea's. Korea's you can get used to. Oh my god. Is still working on this one. You can definitely see it makes multiple creations and tries to change things to make it better. It is kind of improving this robot structure, revealing itself, I think, a little bit. Oh, it's elevating the contrast and the color depth. These scenes are also done 1080p. Taking a look at the detail here in the first one. It's definitely got a little bit of a weird AI texture on the mountains. If you pause this, yeah, you can kind of tell that it's AI generated, especially if you look close. Not to mention all the grass down here, but while it's moving, it's not too too bad. Then it moves down and you get a lot of shimmering because it's trying to do all this detail at once. And it just kind of doesn't get all the way there. Once you get down here though, these flowers, these individual flowers look pretty good. The ones in the background just kind of look like a ton of dots. The motion's good. I see what they're trying to do with the detail, but this is just too much for this scene right here. It's totally becoming a mess by the end. Now, this one is a little bit better for sure. Again, definitely kind of feels a little oversharpened. Looks aesque. As the motion goes down, though, it's not too bad. It's pretty consistent. And then you get the close-up of the flowers. This looks more real. This looks definitely better than the last one we saw. I mean, it's a beautiful scene for sure, but when you try to do long range mountain shots like this, this is not the AI for this type of work. This is definitely more focused on characters and motion and scenes that have things going on. But yeah, I mean, like just off of first glance, this looks pretty cool. I would upscale with something else, though. I just generate in 720p and and use a different upscaler. Okay, apparently these ones are also in HDR, which is pretty cool. Let's download it. So, here is the HDR video as I play it for you guys. You can see, yeah, I mean, it's it's pretty good. There's a lot of detail. You guys can't see this in HDR, but if I go back and just watch the original SDR gen, I don't know. I kind of prefer this. It it it looks a little bit more real, which is kind of the opposite effect of the HDR. It's because it decided to do so much color grading change. If it took this original image and like converted it to HDR without trying to adjust the color and messing with it, I think it would have looked a lot better. Yeah, this one to you guys probably looks all dark and screwed up. I can see more detail than you guys can see. Like over here. It's tough to explain. Well, it does seem to work though as HDR. It just seems like the AI is taking creative liberties that I didn't really ask it to. So, it looks like the robot also arrived in HDR. Let's check this one out in full HDR. You can see these weird shimmering spots down here below. Guys, I can't help but think that these are artifacts caused by it trying to make the video HDR, right? Could be something else. I don't know. I mean, I'm not going to lie, it looks pretty good, especially on like metallic machinery like this. Yeah, that looks cool down over here. You guys again are not going to be able to see it. It is absolutely HDR video. Let's try some more movement. This is going to be a complex one. So, I want basically all these people with their weird lemon juicer helmets to start juicing lemons at the same time. So, we'll do a little twist. Little twist. I don't know if this is really going to convey what we want. You go upwards and twist. Hopefully, it doesn't get confused with that woman. I want all the people wearing lemon juicer helmets to start cranking and rotating the juicers at the same time. I highlighted it in the red arrows for you. We'll say juicers handle. Send that off. That is with Ray 3 reasoning. Did the motion with the jet ski work? Wow, it's kind of like slow motion. That's pretty cool. I wasn't expecting that. This looks like it's the best generation. All right, 1080p. You can see obviously some screwed up words here. Little bit of weird like frame morphing and stuff, but it's cool. It's cool. This isn't supposed to look realistic. We've just got these rotating dolphins flying around with me. It's supposed to be surreal and funny and just, you know, me having a great time with the dolphins. The motion of the water is pretty great overall. Well, definitely some shimmering and weirdness, though. Still, they can't make out each individual water particle yet. I think the dolphins look good. They definitely are kind of following the motion. This one's not following the motion. It did its best with the 5 seconds that it had. I think going for the slow-mo angle is probably saving it. All right, so this one, it kind of treated the dolphins more like they were already in water swimming than in the air. It also kind of changes my face a little bit more and morphs it. Water largely looks the same. The dolphins aren't really following my commands as much. Maybe this one at the bottom is doing a pretty good job, but these guys kind of not really listening up here. So far, this is pretty terrifying. All the juice just starts to spew out of their mouths. They are not cranking the lemon juicers. I think this one's still working, though, so maybe it'll just like correct itself. Here's a really interesting quirk that I just noticed. If I switch to Ray 3 instead of Ray 3 reasoning, that's when I unlock access to actually annotate and draw motion. All right, let's make this ship kind of take off and then zoom on into the stars. The UFO alien craft hovers slightly then gravity boosts fast into the stars. Cinema. All right, the draft of this one looks pretty good. So, based on my initial testing and messing around with this, there are definitely some important takeaways. Is the raw generator under the hood of Ray 3 pretty good? Yeah, I would definitely say it's state-of-the-art. Is it going to beat out the current Kings? I just don't think so. Terms of raw detail and coherency over time. Other models definitely have Ray 3 beat. I will say the improvements to physics are very great. It's definitely more cinematic. It has more detail. It's progress if we're only looking at the luma side of things. Now, adding reasoning on top of this, the reasoning definitely can take a little bit of time. The UI doesn't make it super clear what the AI is exactly doing at that moment, what step in the process it is at, and whether or not it's completely done or not. It will iterate over time. It can convert things to HDR, but oftentimes when you do, it'll do silly filters like this that kind of ruin the vibe, not to mention other artifacts and glitching. Its ability to review and automatically annotate content to attempt to fix it though is very impressive. It's one of the only uses of agent AI video generation that I've seen thus far in totality to be honest with you. And for more simple stuff like this UFO taking off, it does a great job. Like this is super serviceable. I could upscale this as well if I wanted to and it probably look pretty great. But like even the more complex stuff where I'm I'm pointing out arrows, telling it which directions I want dolphins to go into, it definitely can understand and start to push things in the right direction, but it's not consistent and often times it doesn't get all of the way there. I really think they've got a good skeleton and good bones here. They just need to build upon it a little bit more. It seems to struggle carrying consistency and ideas from one chat to the next. When you think about it, the agentic process that they have working under the hood here probably can't go grab this frame again and then start to recreate. So that's why it's generating new ones and then creating this mess. If it could see the whole conversation, go back, grab this frame again, try it again, and also see the videos that it was generating before, then I think we're making more progress to easy simple chat interface style AI video interaction editing and hopefully creating more consistent stories over time. That's something that I think they could also do with this where not just generate one clip for one scenario, generate a bunch of clips with one character. Let's try to make couples of scenes that match and go together. Honestly, I do think the interface they have right now is set up pretty good. It's easy to understand and follow. I can upscale and add audio easily or extend video easily. The beginnings of this chat interface are really good. It's easy to go through and change all of these settings and understand what's going on. And especially this where you can change out keywords is a really great option. So to Lumalabs, I see the vision. I see the potential. I think a little bit of polish and a way for the AI to grab more information when it needs it will take this product up to its next level. Whether or not this is going to be useful for you, the viewer, right now, I think is going to depend on your use case. If you're already creating narrative driven content with AI, and you think that you have a use case for this right now, like, oh, I have this perfect image and this perfect prompt to go along with it, and I want to see if it can do this motion correctly. I think that's where they're going to get users in the door trying this out. But as someone who's on the pulse of this stuff, I'm going to give this one like a B+. I think there's definitely other APIs and video generators that deliver similar or better quality, especially at like 1080p. Their UI definitely is one of a kind and the HDR is as well, but the reasoning here, it's not working like we're used to chat GPT working where it infers things and grabs information that's already in the context. With that being said, I like to see that Luma is pushing and pioneering things in new directions. Let me know what you think down in the comments below, guys, and I'll see you in the next video. Thank you so much for watching.",
          "analysis_source": "transcricao_youtube",
          "summary": {
            "resumo_uma_frase": "Análise do Ray 3 da Lumalabs, um modelo de vídeo com raciocínio, HDR studiograde, modo draft e melhorias de física/consistência, discutindo funcionamento, uso prático e limitações.",
            "resumo": "Rapidamente, o vídeo apresenta o Ray 3 como o suposto primeiro modelo de vídeo com raciocínio, HDR de qualidade de estúdio e modo de rascunho para iteração criativa. O narrador detalha como o sistema observa a cena, gera tentativas, corrige automaticamente falhas e pode ser guiado por anotações visuais. Demonstra cenas com celulares vintage, piratas e Kraken, mostra a diferença entre rascunhos e saídas finais, e discute a exigência de assinatura para ativar o modo de raciocínio. HDR é explicado como calibração de brilho por área, exigindo uma tela compatível. O vídeo também comenta limites práticos, a possibilidade de seleção seletiva e a experiência de uso, sugerindo que a tecnologia é promissora, mas depende da usabilidade real.",
            "assunto_principal": "Avaliação do Ray 3 da Lumalabs: modelo de vídeo com raciocínio, HDR de qualidade de estúdio e avanços no fluxo de trabalho criativo.",
            "palavras_chave": [
              "Ray 3",
              "Lumalabs",
              "inteligência artificial de vídeo",
              "raciocínio visual",
              "HDR de qualidade de estúdio",
              "modo de rascunho",
              "física avançada",
              "consistência",
              "anotações visuais",
              "interface",
              "calibração HDR",
              "SDR",
              "fluxo criativo",
              "personagens consistentes"
            ],
            "resumo_em_topicos": "### Pontos-chave\n- Ray 3 é apresentado como o primeiro modelo de vídeo com raciocínio, HDR de nível estúdio e modo rascunho.\n- O processo de raciocínio envolve análise da cena, geração de rascunhos, identificação de erros e reiteração automática ou com correções via anotações visuais.\n- Demonstração prática: cena com celulares vintage, crítica de consistência (fotos, objetos) e variações de qualidade entre rascunhos e saídas, com a necessidade de assinatura para ativar o modo de raciocínio.\n- HDR: explica que HDR realmente requer tela compatível e calibração, com impacto no visual e cuidado para não converter para SDR.\n- Considerações de uso: seleção seletiva de exemplos, avaliação de usabilidade e promessa de melhoria, dependente da implementação prática.\n- Conclusão: Ray 3 é promissor, mas a adoção depende de facilidade de uso e robustez.",
            "prompt_tokens": 1922,
            "completion_tokens": 3796,
            "model": "gpt-5-nano",
            "cost": 0.0067
          },
          "analysis_time": 68.61361408233643
        }
      ],
      "status": "success"
    }
  ]
}